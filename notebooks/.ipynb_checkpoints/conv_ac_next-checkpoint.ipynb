{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import get_targets, get_data_pair, prepare_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = 'C:\\\\Users\\\\ttanj\\\\UoB\\\\WS18\\\\DBM\\\\data\\data_mmc'\n",
    "HEIGHT = 100\n",
    "WIDTH = 100\n",
    "NUM_CHANNEL = 1\n",
    "RGB = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets():\n",
    "    targets = []\n",
    "    for r in range(1, 4):\n",
    "        for theta in range(0, 190, 30):\n",
    "            theta = np.radians(theta)\n",
    "            targets.append([r * np.cos(theta), r * np.sin(theta)])\n",
    "    return targets\n",
    "\n",
    "\n",
    "def get_data_pair():\n",
    "    data_pair = []\n",
    "    for dirname in os.listdir(BASE_PATH):\n",
    "        fnames = os.listdir(os.path.join(BASE_PATH, dirname))\n",
    "        for j in range(len(fnames) - 1):\n",
    "            data_pair.append((os.path.join(BASE_PATH, dirname, fnames[j]),\n",
    "                              os.path.join(BASE_PATH, dirname, fnames[j + 1])))\n",
    "    return data_pair\n",
    "\n",
    "\n",
    "def prepare_data(num_channel):\n",
    "    data_pair = get_data_pair()\n",
    "    x = np.zeros((len(data_pair), HEIGHT * WIDTH * num_channel))\n",
    "    y = np.zeros((len(data_pair), HEIGHT * WIDTH * num_channel))\n",
    "    \n",
    "    for i, item in zip(range(len(data_pair)), data_pair):\n",
    "        x[i] = cv2.imread(item[0], RGB).flatten()\n",
    "        y[i] = cv2.imread(item[1], RGB).flatten()\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100, 100, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 100, 100, 16)      160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 50, 50, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 50, 50, 8)         1160      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 25, 25, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 25, 25, 8)         584       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 13, 13, 8)         584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 26, 26, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 26, 26, 8)         584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 52, 52, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 50, 50, 16)        1168      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 100, 100, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 100, 100, 1)       145       \n",
      "=================================================================\n",
      "Total params: 4,385\n",
      "Trainable params: 4,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_img = Input(shape=(HEIGHT, WIDTH, NUM_CHANNEL))\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)  # (4, 4, 8) i.e. 128-dimensional\n",
    "\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(NUM_CHANNEL, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 453 samples, validate on 51 samples\n",
      "Epoch 1/200\n",
      "453/453 [==============================] - 21s 46ms/step - loss: 0.5763 - val_loss: 0.2106\n",
      "Epoch 2/200\n",
      "453/453 [==============================] - 21s 47ms/step - loss: 0.2076 - val_loss: 0.1897\n",
      "Epoch 3/200\n",
      "453/453 [==============================] - 21s 46ms/step - loss: 0.1802 - val_loss: 0.1743\n",
      "Epoch 4/200\n",
      "453/453 [==============================] - 23s 50ms/step - loss: 0.1688 - val_loss: 0.1622\n",
      "Epoch 5/200\n",
      "453/453 [==============================] - 21s 45ms/step - loss: 0.1562 - val_loss: 0.1483\n",
      "Epoch 6/200\n",
      "453/453 [==============================] - 23s 50ms/step - loss: 0.1406 - val_loss: 0.1321\n",
      "Epoch 7/200\n",
      "453/453 [==============================] - 30s 65ms/step - loss: 0.1236 - val_loss: 0.1133\n",
      "Epoch 8/200\n",
      "453/453 [==============================] - 34s 75ms/step - loss: 0.1067 - val_loss: 0.0998\n",
      "Epoch 9/200\n",
      "453/453 [==============================] - 34s 76ms/step - loss: 0.0941 - val_loss: 0.0889\n",
      "Epoch 10/200\n",
      "453/453 [==============================] - 33s 73ms/step - loss: 0.0849 - val_loss: 0.0808\n",
      "Epoch 11/200\n",
      "453/453 [==============================] - 27s 60ms/step - loss: 0.0775 - val_loss: 0.0756\n",
      "Epoch 12/200\n",
      "453/453 [==============================] - 29s 64ms/step - loss: 0.0732 - val_loss: 0.0720\n",
      "Epoch 13/200\n",
      "453/453 [==============================] - 29s 64ms/step - loss: 0.0705 - val_loss: 0.0691\n",
      "Epoch 14/200\n",
      "453/453 [==============================] - 24s 52ms/step - loss: 0.0672 - val_loss: 0.0659\n",
      "Epoch 15/200\n",
      "453/453 [==============================] - 23s 51ms/step - loss: 0.0641 - val_loss: 0.0625\n",
      "Epoch 16/200\n",
      "453/453 [==============================] - 26s 57ms/step - loss: 0.0606 - val_loss: 0.0591\n",
      "Epoch 17/200\n",
      "453/453 [==============================] - 27s 59ms/step - loss: 0.0570 - val_loss: 0.0553\n",
      "Epoch 18/200\n",
      "453/453 [==============================] - 29s 65ms/step - loss: 0.0533 - val_loss: 0.0515\n",
      "Epoch 19/200\n",
      "453/453 [==============================] - 28s 61ms/step - loss: 0.0500 - val_loss: 0.0490\n",
      "Epoch 20/200\n",
      "453/453 [==============================] - 28s 61ms/step - loss: 0.0472 - val_loss: 0.0460\n",
      "Epoch 21/200\n",
      "453/453 [==============================] - 29s 63ms/step - loss: 0.0435 - val_loss: 0.0414\n",
      "Epoch 22/200\n",
      "453/453 [==============================] - 30s 66ms/step - loss: 0.0389 - val_loss: 0.0364\n",
      "Epoch 23/200\n",
      "453/453 [==============================] - 29s 65ms/step - loss: 0.0341 - val_loss: 0.0324\n",
      "Epoch 24/200\n",
      "453/453 [==============================] - 28s 61ms/step - loss: 0.0320 - val_loss: 0.0310\n",
      "Epoch 25/200\n",
      "453/453 [==============================] - 28s 62ms/step - loss: 0.0309 - val_loss: 0.0305\n",
      "Epoch 26/200\n",
      "453/453 [==============================] - 28s 62ms/step - loss: 0.0303 - val_loss: 0.0301\n",
      "Epoch 27/200\n",
      "453/453 [==============================] - 31s 68ms/step - loss: 0.0300 - val_loss: 0.0305\n",
      "Epoch 28/200\n",
      "453/453 [==============================] - 31s 68ms/step - loss: 0.0297 - val_loss: 0.0294\n",
      "Epoch 29/200\n",
      "453/453 [==============================] - 24s 53ms/step - loss: 0.0294 - val_loss: 0.0295\n",
      "Epoch 30/200\n",
      "453/453 [==============================] - 29s 64ms/step - loss: 0.0294 - val_loss: 0.0291\n",
      "Epoch 31/200\n",
      "453/453 [==============================] - 32s 71ms/step - loss: 0.0293 - val_loss: 0.0291\n",
      "Epoch 32/200\n",
      "453/453 [==============================] - 34s 76ms/step - loss: 0.0291 - val_loss: 0.0289\n",
      "Epoch 33/200\n",
      "453/453 [==============================] - 34s 76ms/step - loss: 0.0289 - val_loss: 0.0291\n",
      "Epoch 34/200\n",
      "453/453 [==============================] - 30s 65ms/step - loss: 0.0289 - val_loss: 0.0289\n",
      "Epoch 35/200\n",
      "453/453 [==============================] - 29s 63ms/step - loss: 0.0287 - val_loss: 0.0285\n",
      "Epoch 36/200\n",
      "453/453 [==============================] - 32s 71ms/step - loss: 0.0286 - val_loss: 0.0294\n",
      "Epoch 37/200\n",
      "453/453 [==============================] - 30s 67ms/step - loss: 0.0286 - val_loss: 0.0286\n",
      "Epoch 38/200\n",
      "453/453 [==============================] - 29s 64ms/step - loss: 0.0284 - val_loss: 0.0283\n",
      "Epoch 39/200\n",
      "453/453 [==============================] - 32s 70ms/step - loss: 0.0284 - val_loss: 0.0283\n",
      "Epoch 40/200\n",
      "453/453 [==============================] - 30s 66ms/step - loss: 0.0284 - val_loss: 0.0286\n",
      "Epoch 41/200\n",
      "453/453 [==============================] - 31s 69ms/step - loss: 0.0285 - val_loss: 0.0281\n",
      "Epoch 42/200\n",
      "453/453 [==============================] - 29s 64ms/step - loss: 0.0282 - val_loss: 0.0283\n",
      "Epoch 43/200\n",
      "453/453 [==============================] - 26s 58ms/step - loss: 0.0281 - val_loss: 0.0280\n",
      "Epoch 44/200\n",
      "453/453 [==============================] - 26s 58ms/step - loss: 0.0282 - val_loss: 0.0284\n",
      "Epoch 45/200\n",
      "453/453 [==============================] - 26s 57ms/step - loss: 0.0281 - val_loss: 0.0283\n",
      "Epoch 46/200\n",
      "453/453 [==============================] - 26s 56ms/step - loss: 0.0279 - val_loss: 0.0279\n",
      "Epoch 47/200\n",
      "453/453 [==============================] - 28s 61ms/step - loss: 0.0278 - val_loss: 0.0279\n",
      "Epoch 48/200\n",
      "453/453 [==============================] - 25s 55ms/step - loss: 0.0278 - val_loss: 0.0277\n",
      "Epoch 49/200\n",
      "453/453 [==============================] - 25s 55ms/step - loss: 0.0277 - val_loss: 0.0279\n",
      "Epoch 50/200\n",
      "453/453 [==============================] - 26s 56ms/step - loss: 0.0277 - val_loss: 0.0276\n",
      "Epoch 51/200\n",
      "453/453 [==============================] - 30s 65ms/step - loss: 0.0276 - val_loss: 0.0275\n",
      "Epoch 52/200\n",
      "453/453 [==============================] - 31s 69ms/step - loss: 0.0275 - val_loss: 0.0277\n",
      "Epoch 53/200\n",
      "453/453 [==============================] - 32s 72ms/step - loss: 0.0274 - val_loss: 0.0275\n",
      "Epoch 54/200\n",
      "453/453 [==============================] - 30s 67ms/step - loss: 0.0275 - val_loss: 0.0276\n",
      "Epoch 55/200\n",
      "453/453 [==============================] - 27s 59ms/step - loss: 0.0274 - val_loss: 0.0273\n",
      "Epoch 56/200\n",
      "453/453 [==============================] - 33s 72ms/step - loss: 0.0274 - val_loss: 0.0272\n",
      "Epoch 57/200\n",
      "453/453 [==============================] - 35s 78ms/step - loss: 0.0274 - val_loss: 0.0275\n",
      "Epoch 58/200\n",
      "453/453 [==============================] - 31s 67ms/step - loss: 0.0273 - val_loss: 0.0271\n",
      "Epoch 59/200\n",
      "453/453 [==============================] - 32s 70ms/step - loss: 0.0273 - val_loss: 0.0275\n",
      "Epoch 60/200\n",
      "453/453 [==============================] - 34s 74ms/step - loss: 0.0274 - val_loss: 0.0271\n",
      "Epoch 61/200\n",
      "453/453 [==============================] - 27s 59ms/step - loss: 0.0272 - val_loss: 0.0272\n",
      "Epoch 62/200\n",
      "453/453 [==============================] - 29s 64ms/step - loss: 0.0275 - val_loss: 0.0271\n",
      "Epoch 63/200\n",
      "453/453 [==============================] - 26s 57ms/step - loss: 0.0270 - val_loss: 0.0269\n",
      "Epoch 64/200\n",
      "453/453 [==============================] - 25s 55ms/step - loss: 0.0270 - val_loss: 0.0269\n",
      "Epoch 65/200\n",
      "453/453 [==============================] - 26s 56ms/step - loss: 0.0269 - val_loss: 0.0271\n",
      "Epoch 66/200\n",
      "453/453 [==============================] - 23s 50ms/step - loss: 0.0270 - val_loss: 0.0269\n",
      "Epoch 67/200\n",
      "453/453 [==============================] - 24s 53ms/step - loss: 0.0272 - val_loss: 0.0268\n",
      "Epoch 68/200\n",
      "453/453 [==============================] - 26s 56ms/step - loss: 0.0270 - val_loss: 0.0269\n",
      "Epoch 69/200\n",
      "300/453 [==================>...........] - ETA: 10s - loss: 0.0268Epoch 74/200\n",
      "453/453 [==============================] - 26s 58ms/step - loss: 0.0270 - val_loss: 0.0266\n",
      "Epoch 75/200\n",
      "453/453 [==============================] - 26s 58ms/step - loss: 0.0269 - val_loss: 0.0267\n",
      "Epoch 76/200\n",
      "453/453 [==============================] - 23s 51ms/step - loss: 0.0267 - val_loss: 0.0266\n",
      "Epoch 77/200\n",
      "453/453 [==============================] - 24s 52ms/step - loss: 0.0266 - val_loss: 0.0265\n",
      "Epoch 78/200\n",
      "453/453 [==============================] - 22s 49ms/step - loss: 0.0267 - val_loss: 0.0265\n",
      "Epoch 79/200\n",
      "453/453 [==============================] - 23s 50ms/step - loss: 0.0266 - val_loss: 0.0266\n",
      "Epoch 80/200\n",
      "453/453 [==============================] - 25s 55ms/step - loss: 0.0265 - val_loss: 0.0265\n",
      "Epoch 81/200\n",
      "453/453 [==============================] - 26s 57ms/step - loss: 0.0265 - val_loss: 0.0265\n",
      "Epoch 82/200\n",
      "453/453 [==============================] - 28s 61ms/step - loss: 0.0264 - val_loss: 0.0264\n",
      "Epoch 83/200\n",
      "453/453 [==============================] - 24s 54ms/step - loss: 0.0266 - val_loss: 0.0266\n",
      "Epoch 84/200\n",
      "453/453 [==============================] - 24s 53ms/step - loss: 0.0265 - val_loss: 0.0264\n",
      "Epoch 85/200\n",
      "453/453 [==============================] - 25s 54ms/step - loss: 0.0265 - val_loss: 0.0264\n",
      "Epoch 86/200\n",
      "453/453 [==============================] - 25s 54ms/step - loss: 0.0265 - val_loss: 0.0266\n",
      "Epoch 87/200\n",
      "453/453 [==============================] - 24s 54ms/step - loss: 0.0267 - val_loss: 0.0268\n",
      "Epoch 88/200\n",
      "453/453 [==============================] - 24s 53ms/step - loss: 0.0266 - val_loss: 0.0263\n",
      "Epoch 89/200\n",
      "453/453 [==============================] - 25s 54ms/step - loss: 0.0264 - val_loss: 0.0263\n",
      "Epoch 98/200\n",
      "453/453 [==============================] - 28s 62ms/step - loss: 0.0263 - val_loss: 0.0262\n",
      "Epoch 99/200\n",
      "453/453 [==============================] - 25s 55ms/step - loss: 0.0263 - val_loss: 0.0262\n",
      "Epoch 100/200\n",
      "453/453 [==============================] - 24s 53ms/step - loss: 0.0262 - val_loss: 0.0262\n",
      "Epoch 101/200\n",
      "453/453 [==============================] - 23s 51ms/step - loss: 0.0262 - val_loss: 0.0262\n",
      "Epoch 102/200\n",
      "453/453 [==============================] - 23s 52ms/step - loss: 0.0262 - val_loss: 0.0265\n",
      "Epoch 103/200\n",
      "453/453 [==============================] - 24s 52ms/step - loss: 0.0262 - val_loss: 0.0263\n",
      "Epoch 104/200\n",
      "453/453 [==============================] - 23s 50ms/step - loss: 0.0263 - val_loss: 0.0265\n",
      "Epoch 105/200\n",
      "453/453 [==============================] - 23s 50ms/step - loss: 0.0262 - val_loss: 0.0263\n",
      "Epoch 106/200\n",
      "453/453 [==============================] - 25s 54ms/step - loss: 0.0262 - val_loss: 0.0262\n",
      "Epoch 107/200\n",
      "453/453 [==============================] - 25s 55ms/step - loss: 0.0261 - val_loss: 0.0262\n",
      "Epoch 108/200\n",
      "453/453 [==============================] - 26s 56ms/step - loss: 0.0265 - val_loss: 0.0261\n",
      "Epoch 109/200\n",
      "453/453 [==============================] - 24s 52ms/step - loss: 0.0261 - val_loss: 0.0261\n",
      "Epoch 110/200\n",
      "453/453 [==============================] - 24s 54ms/step - loss: 0.0262 - val_loss: 0.0264\n",
      "Epoch 111/200\n",
      "453/453 [==============================] - 23s 50ms/step - loss: 0.0262 - val_loss: 0.0262\n",
      "Epoch 112/200\n",
      "453/453 [==============================] - 22s 48ms/step - loss: 0.0260 - val_loss: 0.0262\n",
      "Epoch 113/200\n",
      "453/453 [==============================] - 21s 47ms/step - loss: 0.0261 - val_loss: 0.0261\n",
      "Epoch 114/200\n",
      "453/453 [==============================] - 21s 47ms/step - loss: 0.0261 - val_loss: 0.0262\n",
      "Epoch 115/200\n",
      "453/453 [==============================] - 22s 48ms/step - loss: 0.0261 - val_loss: 0.0260\n",
      "Epoch 116/200\n",
      "453/453 [==============================] - 22s 49ms/step - loss: 0.0261 - val_loss: 0.0260\n",
      "Epoch 117/200\n",
      "453/453 [==============================] - 22s 48ms/step - loss: 0.0260 - val_loss: 0.0260\n",
      "Epoch 118/200\n",
      "453/453 [==============================] - 21s 47ms/step - loss: 0.0262 - val_loss: 0.0262\n",
      "Epoch 119/200\n",
      "453/453 [==============================] - 21s 47ms/step - loss: 0.0260 - val_loss: 0.0260\n",
      "Epoch 120/200\n",
      "453/453 [==============================] - 22s 48ms/step - loss: 0.0260 - val_loss: 0.0261\n",
      "Epoch 121/200\n",
      "453/453 [==============================] - 22s 48ms/step - loss: 0.0260 - val_loss: 0.0265\n",
      "Epoch 122/200\n",
      "453/453 [==============================] - 21s 47ms/step - loss: 0.0261 - val_loss: 0.0263\n",
      "Epoch 123/200\n",
      "453/453 [==============================] - 22s 48ms/step - loss: 0.0262 - val_loss: 0.0260\n",
      "Epoch 124/200\n",
      "453/453 [==============================] - 22s 50ms/step - loss: 0.0260 - val_loss: 0.0260\n",
      "Epoch 125/200\n",
      "453/453 [==============================] - 24s 53ms/step - loss: 0.0260 - val_loss: 0.0262\n",
      "Epoch 126/200\n",
      "453/453 [==============================] - 24s 53ms/step - loss: 0.0259 - val_loss: 0.0260\n",
      "Epoch 127/200\n",
      "453/453 [==============================] - 22s 48ms/step - loss: 0.0259 - val_loss: 0.0260\n",
      "Epoch 128/200\n",
      "453/453 [==============================] - 23s 51ms/step - loss: 0.0258 - val_loss: 0.0260\n",
      "Epoch 129/200\n",
      "453/453 [==============================] - 26s 57ms/step - loss: 0.0259 - val_loss: 0.0262\n",
      "Epoch 130/200\n",
      "453/453 [==============================] - 23s 51ms/step - loss: 0.0260 - val_loss: 0.0260\n",
      "Epoch 131/200\n",
      "453/453 [==============================] - 24s 53ms/step - loss: 0.0259 - val_loss: 0.0259\n",
      "Epoch 132/200\n",
      "453/453 [==============================] - 21s 47ms/step - loss: 0.0259 - val_loss: 0.0260\n",
      "Epoch 133/200\n",
      "453/453 [==============================] - 21s 47ms/step - loss: 0.0260 - val_loss: 0.0263\n",
      "Epoch 134/200\n",
      "453/453 [==============================] - 25s 55ms/step - loss: 0.0263 - val_loss: 0.0260\n",
      "Epoch 135/200\n",
      "453/453 [==============================] - 23s 51ms/step - loss: 0.0259 - val_loss: 0.0260\n",
      "Epoch 136/200\n",
      "453/453 [==============================] - 21s 47ms/step - loss: 0.0259 - val_loss: 0.0260\n",
      "Epoch 137/200\n",
      "453/453 [==============================] - 23s 51ms/step - loss: 0.0259 - val_loss: 0.0261\n",
      "Epoch 138/200\n",
      "453/453 [==============================] - 24s 54ms/step - loss: 0.0258 - val_loss: 0.0259\n",
      "Epoch 139/200\n",
      "453/453 [==============================] - 25s 55ms/step - loss: 0.0258 - val_loss: 0.0261\n",
      "Epoch 140/200\n",
      "453/453 [==============================] - 23s 51ms/step - loss: 0.0259 - val_loss: 0.0262\n",
      "Epoch 141/200\n",
      "453/453 [==============================] - 23s 50ms/step - loss: 0.0256 - val_loss: 0.0258\n",
      "Epoch 156/200\n",
      "453/453 [==============================] - 23s 51ms/step - loss: 0.0258 - val_loss: 0.0258\n",
      "Epoch 157/200\n",
      "453/453 [==============================] - 22s 50ms/step - loss: 0.0257 - val_loss: 0.0259\n",
      "Epoch 158/200\n",
      "453/453 [==============================] - 23s 51ms/step - loss: 0.0257 - val_loss: 0.0258\n",
      "Epoch 159/200\n",
      "453/453 [==============================] - 23s 52ms/step - loss: 0.0257 - val_loss: 0.0258\n",
      "Epoch 160/200\n",
      "453/453 [==============================] - 22s 49ms/step - loss: 0.0257 - val_loss: 0.0260\n",
      "Epoch 161/200\n",
      "453/453 [==============================] - 23s 50ms/step - loss: 0.0257 - val_loss: 0.0257\n",
      "Epoch 162/200\n",
      "453/453 [==============================] - 23s 50ms/step - loss: 0.0256 - val_loss: 0.0257\n",
      "Epoch 163/200\n",
      "453/453 [==============================] - 23s 51ms/step - loss: 0.0256 - val_loss: 0.0258\n",
      "Epoch 164/200\n",
      "453/453 [==============================] - 21s 45ms/step - loss: 0.0256 - val_loss: 0.0260\n",
      "Epoch 165/200\n",
      "453/453 [==============================] - 21s 47ms/step - loss: 0.0256 - val_loss: 0.0257\n",
      "Epoch 166/200\n",
      "453/453 [==============================] - 21s 47ms/step - loss: 0.0256 - val_loss: 0.0257\n",
      "Epoch 167/200\n",
      "453/453 [==============================] - 21s 46ms/step - loss: 0.0256 - val_loss: 0.0258\n",
      "Epoch 168/200\n",
      "453/453 [==============================] - 21s 46ms/step - loss: 0.0256 - val_loss: 0.0257\n",
      "Epoch 169/200\n",
      "453/453 [==============================] - 22s 49ms/step - loss: 0.0256 - val_loss: 0.0257\n",
      "Epoch 172/200\n",
      "453/453 [==============================] - 24s 53ms/step - loss: 0.0255 - val_loss: 0.0257\n",
      "Epoch 173/200\n",
      "453/453 [==============================] - 24s 53ms/step - loss: 0.0255 - val_loss: 0.0257\n",
      "Epoch 174/200\n",
      "453/453 [==============================] - 25s 54ms/step - loss: 0.0256 - val_loss: 0.0257\n",
      "Epoch 175/200\n",
      "453/453 [==============================] - 25s 55ms/step - loss: 0.0255 - val_loss: 0.0257\n",
      "Epoch 176/200\n",
      "453/453 [==============================] - 21s 47ms/step - loss: 0.0255 - val_loss: 0.0256\n",
      "Epoch 177/200\n",
      "453/453 [==============================] - 21s 47ms/step - loss: 0.0255 - val_loss: 0.0256\n",
      "Epoch 178/200\n",
      "453/453 [==============================] - 21s 47ms/step - loss: 0.0255 - val_loss: 0.0257\n",
      "Epoch 179/200\n",
      "453/453 [==============================] - 21s 46ms/step - loss: 0.0256 - val_loss: 0.0257\n",
      "Epoch 180/200\n",
      "453/453 [==============================] - 21s 46ms/step - loss: 0.0255 - val_loss: 0.0256\n",
      "Epoch 181/200\n",
      "453/453 [==============================] - 22s 48ms/step - loss: 0.0255 - val_loss: 0.0256\n",
      "Epoch 182/200\n",
      "453/453 [==============================] - 21s 47ms/step - loss: 0.0255 - val_loss: 0.0259\n",
      "Epoch 183/200\n",
      "453/453 [==============================] - 21s 47ms/step - loss: 0.0256 - val_loss: 0.0256\n",
      "Epoch 184/200\n",
      "453/453 [==============================] - 21s 46ms/step - loss: 0.0254 - val_loss: 0.0261\n",
      "Epoch 185/200\n",
      "453/453 [==============================] - 21s 47ms/step - loss: 0.0255 - val_loss: 0.0256\n",
      "Epoch 186/200\n",
      "453/453 [==============================] - 21s 47ms/step - loss: 0.0255 - val_loss: 0.0255\n",
      "Epoch 187/200\n",
      "453/453 [==============================] - 21s 47ms/step - loss: 0.0255 - val_loss: 0.0258\n",
      "Epoch 188/200\n",
      "453/453 [==============================] - 21s 47ms/step - loss: 0.0256 - val_loss: 0.0256\n",
      "Epoch 189/200\n",
      "453/453 [==============================] - 22s 48ms/step - loss: 0.0254 - val_loss: 0.0256\n",
      "Epoch 190/200\n",
      "453/453 [==============================] - 22s 48ms/step - loss: 0.0254 - val_loss: 0.0255\n",
      "Epoch 191/200\n",
      "453/453 [==============================] - 21s 47ms/step - loss: 0.0256 - val_loss: 0.0256\n",
      "Epoch 192/200\n",
      "453/453 [==============================] - 21s 46ms/step - loss: 0.0255 - val_loss: 0.0255\n",
      "Epoch 193/200\n",
      "453/453 [==============================] - 18s 40ms/step - loss: 0.0256 - val_loss: 0.0262\n",
      "Epoch 194/200\n",
      "453/453 [==============================] - 12s 27ms/step - loss: 0.0256 - val_loss: 0.0256\n",
      "Epoch 195/200\n",
      "453/453 [==============================] - 12s 27ms/step - loss: 0.0255 - val_loss: 0.0259\n",
      "Epoch 196/200\n",
      "453/453 [==============================] - 12s 27ms/step - loss: 0.0256 - val_loss: 0.0255\n",
      "Epoch 197/200\n",
      "453/453 [==============================] - 12s 27ms/step - loss: 0.0254 - val_loss: 0.0258\n",
      "Epoch 198/200\n",
      "453/453 [==============================] - 12s 27ms/step - loss: 0.0255 - val_loss: 0.0257\n",
      "Epoch 199/200\n",
      "453/453 [==============================] - 12s 27ms/step - loss: 0.0254 - val_loss: 0.0255\n",
      "Epoch 200/200\n",
      "453/453 [==============================] - 12s 27ms/step - loss: 0.0254 - val_loss: 0.0255\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23a94639710>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = prepare_data(NUM_CHANNEL)\n",
    "\n",
    "x = x / 255.\n",
    "y = y / 255.\n",
    "\n",
    "x = x.reshape((-1, HEIGHT, WIDTH, NUM_CHANNEL))\n",
    "y = y.reshape((-1, HEIGHT, WIDTH, NUM_CHANNEL))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)\n",
    "\n",
    "autoencoder.fit(x_train, y_train,\n",
    "                epochs=200,\n",
    "                batch_size=20,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs = autoencoder.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 5\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# display original\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "plt.imshow(y_test[idx].reshape(HEIGHT, WIDTH))\n",
    "plt.gray()\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "\n",
    "# display reconstruction\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "plt.imshow(decoded_imgs[idx].reshape(HEIGHT, WIDTH))\n",
    "plt.gray()\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
