{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-1aa30fa84c2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdata_generator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataGen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'data_generator'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from data_generator import DataGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/data_mmc/'\n",
    "HEIGHT = 100\n",
    "WIDTH = 100\n",
    "NUM_CHANNEL = 1\n",
    "RGB = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets():\n",
    "    targets = []\n",
    "    for r in range(1, 4):\n",
    "        for theta in range(0, 190, 30):\n",
    "            theta = np.radians(theta)\n",
    "            targets.append([r * np.cos(theta), r * np.sin(theta)])\n",
    "    return targets\n",
    "\n",
    "\n",
    "def get_data_pair():\n",
    "    data_pair = []\n",
    "    for dirname in os.listdir(BASE_PATH):\n",
    "        fnames = os.listdir(os.path.join(BASE_PATH, dirname))\n",
    "        for j in range(len(fnames) - 1):\n",
    "            data_pair.append((os.path.join(BASE_PATH, dirname, fnames[j]),\n",
    "                              os.path.join(BASE_PATH, dirname, fnames[j + 1])))\n",
    "    return data_pair\n",
    "\n",
    "\n",
    "def prepare_data(num_channel):\n",
    "    data_pair = get_data_pair()\n",
    "    x = np.zeros((len(data_pair), HEIGHT * WIDTH * num_channel))\n",
    "    y = np.zeros((len(data_pair), HEIGHT * WIDTH * num_channel))\n",
    "    \n",
    "    for i, item in zip(range(len(data_pair)), data_pair):\n",
    "        x[i] = cv2.imread(item[0], RGB).flatten()\n",
    "        y[i] = cv2.imread(item[1], RGB).flatten()\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100, 100, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 100, 100, 16)      160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 50, 50, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 50, 50, 8)         1160      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 25, 25, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 25, 25, 8)         584       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 13, 13, 8)         584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 26, 26, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 26, 26, 8)         584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 52, 52, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 50, 50, 16)        1168      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 100, 100, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 100, 100, 1)       145       \n",
      "=================================================================\n",
      "Total params: 4,385\n",
      "Trainable params: 4,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_img = Input(shape=(HEIGHT, WIDTH, NUM_CHANNEL))\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)  # (4, 4, 8) i.e. 128-dimensional\n",
    "\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(NUM_CHANNEL, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgen = Da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 453 samples, validate on 51 samples\n",
      "Epoch 1/200\n",
      "453/453 [==============================] - 11s 25ms/step - loss: 0.6713 - val_loss: 0.5976\n",
      "Epoch 2/200\n",
      "453/453 [==============================] - 11s 24ms/step - loss: 0.3359 - val_loss: 0.2028\n",
      "Epoch 3/200\n",
      "453/453 [==============================] - 12s 26ms/step - loss: 0.1900 - val_loss: 0.1823\n",
      "Epoch 4/200\n",
      "453/453 [==============================] - 11s 25ms/step - loss: 0.1741 - val_loss: 0.1650\n",
      "Epoch 5/200\n",
      "453/453 [==============================] - 11s 25ms/step - loss: 0.1545 - val_loss: 0.1400\n",
      "Epoch 6/200\n",
      "453/453 [==============================] - 11s 24ms/step - loss: 0.1236 - val_loss: 0.1086\n",
      "Epoch 7/200\n",
      "453/453 [==============================] - 12s 26ms/step - loss: 0.1002 - val_loss: 0.0916\n",
      "Epoch 8/200\n",
      "453/453 [==============================] - 18s 40ms/step - loss: 0.0852 - val_loss: 0.0774\n",
      "Epoch 9/200\n",
      "453/453 [==============================] - 21s 46ms/step - loss: 0.0707 - val_loss: 0.0620\n",
      "Epoch 10/200\n",
      "453/453 [==============================] - 21s 45ms/step - loss: 0.0559 - val_loss: 0.0506\n",
      "Epoch 11/200\n",
      "453/453 [==============================] - 23s 50ms/step - loss: 0.0470 - val_loss: 0.0435\n",
      "Epoch 12/200\n",
      "453/453 [==============================] - 20s 45ms/step - loss: 0.0405 - val_loss: 0.0375\n",
      "Epoch 13/200\n",
      "453/453 [==============================] - 22s 49ms/step - loss: 0.0359 - val_loss: 0.0341\n",
      "Epoch 14/200\n",
      "453/453 [==============================] - 29s 63ms/step - loss: 0.0334 - val_loss: 0.0326\n",
      "Epoch 15/200\n",
      "453/453 [==============================] - 34s 74ms/step - loss: 0.0321 - val_loss: 0.0315\n",
      "Epoch 16/200\n",
      "453/453 [==============================] - 35s 78ms/step - loss: 0.0313 - val_loss: 0.0310\n",
      "Epoch 17/200\n",
      "453/453 [==============================] - 32s 71ms/step - loss: 0.0308 - val_loss: 0.0306\n",
      "Epoch 18/200\n",
      "453/453 [==============================] - 28s 62ms/step - loss: 0.0304 - val_loss: 0.0300\n",
      "Epoch 19/200\n",
      "453/453 [==============================] - 28s 63ms/step - loss: 0.0301 - val_loss: 0.0297\n",
      "Epoch 20/200\n",
      "453/453 [==============================] - 28s 63ms/step - loss: 0.0297 - val_loss: 0.0296\n",
      "Epoch 21/200\n",
      "453/453 [==============================] - 24s 54ms/step - loss: 0.0295 - val_loss: 0.0292\n",
      "Epoch 22/200\n",
      "453/453 [==============================] - 23s 51ms/step - loss: 0.0292 - val_loss: 0.0290\n",
      "Epoch 23/200\n",
      "453/453 [==============================] - 25s 55ms/step - loss: 0.0290 - val_loss: 0.0289\n",
      "Epoch 24/200\n",
      "453/453 [==============================] - 28s 61ms/step - loss: 0.0289 - val_loss: 0.0288\n",
      "Epoch 25/200\n",
      "453/453 [==============================] - 28s 61ms/step - loss: 0.0287 - val_loss: 0.0286\n",
      "Epoch 26/200\n",
      "453/453 [==============================] - 29s 63ms/step - loss: 0.0285 - val_loss: 0.0284\n",
      "Epoch 27/200\n",
      "453/453 [==============================] - 27s 59ms/step - loss: 0.0284 - val_loss: 0.0282\n",
      "Epoch 28/200\n",
      "453/453 [==============================] - 28s 61ms/step - loss: 0.0283 - val_loss: 0.0282\n",
      "Epoch 29/200\n",
      "453/453 [==============================] - 31s 68ms/step - loss: 0.0282 - val_loss: 0.0279\n",
      "Epoch 30/200\n",
      "453/453 [==============================] - 30s 67ms/step - loss: 0.0280 - val_loss: 0.0279\n",
      "Epoch 31/200\n",
      "453/453 [==============================] - 29s 64ms/step - loss: 0.0279 - val_loss: 0.0278\n",
      "Epoch 32/200\n",
      "453/453 [==============================] - 27s 60ms/step - loss: 0.0278 - val_loss: 0.0277\n",
      "Epoch 33/200\n",
      "453/453 [==============================] - 28s 62ms/step - loss: 0.0276 - val_loss: 0.0275\n",
      "Epoch 34/200\n",
      "453/453 [==============================] - 31s 68ms/step - loss: 0.0276 - val_loss: 0.0274\n",
      "Epoch 35/200\n",
      "453/453 [==============================] - 30s 66ms/step - loss: 0.0274 - val_loss: 0.0276\n",
      "Epoch 36/200\n",
      "453/453 [==============================] - 26s 57ms/step - loss: 0.0274 - val_loss: 0.0273\n",
      "Epoch 37/200\n",
      "453/453 [==============================] - 28s 63ms/step - loss: 0.0273 - val_loss: 0.0272\n",
      "Epoch 38/200\n",
      "453/453 [==============================] - 32s 70ms/step - loss: 0.0271 - val_loss: 0.0271\n",
      "Epoch 39/200\n",
      "453/453 [==============================] - 34s 74ms/step - loss: 0.0270 - val_loss: 0.0271\n",
      "Epoch 40/200\n",
      "453/453 [==============================] - 33s 72ms/step - loss: 0.0270 - val_loss: 0.0269\n",
      "Epoch 41/200\n",
      "453/453 [==============================] - 31s 69ms/step - loss: 0.0268 - val_loss: 0.0270\n",
      "Epoch 42/200\n",
      "453/453 [==============================] - 27s 59ms/step - loss: 0.0269 - val_loss: 0.0269\n",
      "Epoch 43/200\n",
      "453/453 [==============================] - 32s 71ms/step - loss: 0.0268 - val_loss: 0.0267\n",
      "Epoch 44/200\n",
      "453/453 [==============================] - 31s 68ms/step - loss: 0.0266 - val_loss: 0.0267\n",
      "Epoch 45/200\n",
      "453/453 [==============================] - 30s 66ms/step - loss: 0.0266 - val_loss: 0.0267\n",
      "Epoch 46/200\n",
      "453/453 [==============================] - 31s 69ms/step - loss: 0.0265 - val_loss: 0.0265\n",
      "Epoch 47/200\n",
      "453/453 [==============================] - 30s 67ms/step - loss: 0.0265 - val_loss: 0.0264\n",
      "Epoch 48/200\n",
      "453/453 [==============================] - 32s 70ms/step - loss: 0.0264 - val_loss: 0.0264\n",
      "Epoch 49/200\n",
      "453/453 [==============================] - 29s 65ms/step - loss: 0.0264 - val_loss: 0.0271\n",
      "Epoch 50/200\n",
      "453/453 [==============================] - 27s 60ms/step - loss: 0.0267 - val_loss: 0.0266\n",
      "Epoch 51/200\n",
      "453/453 [==============================] - 27s 59ms/step - loss: 0.0264 - val_loss: 0.0263\n",
      "Epoch 52/200\n",
      "453/453 [==============================] - 26s 56ms/step - loss: 0.0262 - val_loss: 0.0264\n",
      "Epoch 53/200\n",
      "453/453 [==============================] - 25s 55ms/step - loss: 0.0262 - val_loss: 0.0262\n",
      "Epoch 54/200\n",
      "453/453 [==============================] - 29s 63ms/step - loss: 0.0261 - val_loss: 0.0262\n",
      "Epoch 55/200\n",
      "453/453 [==============================] - 25s 55ms/step - loss: 0.0261 - val_loss: 0.0263\n",
      "Epoch 56/200\n",
      "453/453 [==============================] - 25s 56ms/step - loss: 0.0261 - val_loss: 0.0262\n",
      "Epoch 57/200\n",
      "453/453 [==============================] - 26s 56ms/step - loss: 0.0260 - val_loss: 0.0260\n",
      "Epoch 58/200\n",
      "453/453 [==============================] - 28s 61ms/step - loss: 0.0259 - val_loss: 0.0260\n",
      "Epoch 59/200\n",
      "453/453 [==============================] - 32s 70ms/step - loss: 0.0259 - val_loss: 0.0259\n",
      "Epoch 60/200\n",
      "453/453 [==============================] - 33s 74ms/step - loss: 0.0259 - val_loss: 0.0260\n",
      "Epoch 61/200\n",
      "453/453 [==============================] - 30s 65ms/step - loss: 0.0259 - val_loss: 0.0259\n",
      "Epoch 62/200\n",
      "453/453 [==============================] - 28s 61ms/step - loss: 0.0260 - val_loss: 0.0258\n",
      "Epoch 63/200\n",
      "453/453 [==============================] - 30s 66ms/step - loss: 0.0259 - val_loss: 0.0258\n",
      "Epoch 64/200\n",
      "453/453 [==============================] - 37s 81ms/step - loss: 0.0258 - val_loss: 0.0258\n",
      "Epoch 65/200\n",
      "453/453 [==============================] - 32s 70ms/step - loss: 0.0257 - val_loss: 0.0259\n",
      "Epoch 66/200\n",
      "453/453 [==============================] - 31s 69ms/step - loss: 0.0257 - val_loss: 0.0259\n",
      "Epoch 67/200\n",
      "453/453 [==============================] - 34s 76ms/step - loss: 0.0257 - val_loss: 0.0260\n",
      "Epoch 68/200\n",
      "453/453 [==============================] - 27s 60ms/step - loss: 0.0258 - val_loss: 0.0257\n",
      "Epoch 69/200\n",
      "453/453 [==============================] - 28s 62ms/step - loss: 0.0256 - val_loss: 0.0257\n",
      "Epoch 70/200\n",
      "453/453 [==============================] - 28s 61ms/step - loss: 0.0256 - val_loss: 0.0257\n",
      "Epoch 71/200\n",
      "453/453 [==============================] - 24s 53ms/step - loss: 0.0255 - val_loss: 0.0256\n",
      "Epoch 72/200\n",
      "453/453 [==============================] - 26s 56ms/step - loss: 0.0255 - val_loss: 0.0256\n",
      "Epoch 73/200\n",
      "453/453 [==============================] - 23s 50ms/step - loss: 0.0256 - val_loss: 0.0256\n",
      "Epoch 74/200\n",
      "453/453 [==============================] - 23s 51ms/step - loss: 0.0255 - val_loss: 0.0255\n",
      "Epoch 75/200\n",
      "453/453 [==============================] - 26s 58ms/step - loss: 0.0255 - val_loss: 0.0255\n",
      "Epoch 76/200\n",
      "453/453 [==============================] - 30s 66ms/step - loss: 0.0254 - val_loss: 0.0257\n",
      "Epoch 77/200\n",
      "453/453 [==============================] - 28s 61ms/step - loss: 0.0256 - val_loss: 0.0255\n",
      "Epoch 78/200\n",
      "453/453 [==============================] - 25s 56ms/step - loss: 0.0254 - val_loss: 0.0255\n",
      "Epoch 79/200\n",
      "453/453 [==============================] - 26s 58ms/step - loss: 0.0255 - val_loss: 0.0255\n",
      "Epoch 80/200\n",
      "453/453 [==============================] - 26s 57ms/step - loss: 0.0255 - val_loss: 0.0256\n",
      "Epoch 81/200\n",
      "453/453 [==============================] - 27s 59ms/step - loss: 0.0253 - val_loss: 0.0256\n",
      "Epoch 82/200\n",
      "453/453 [==============================] - 27s 59ms/step - loss: 0.0253 - val_loss: 0.0254\n",
      "Epoch 83/200\n",
      "453/453 [==============================] - 23s 51ms/step - loss: 0.0253 - val_loss: 0.0258\n",
      "Epoch 84/200\n",
      "453/453 [==============================] - 24s 52ms/step - loss: 0.0253 - val_loss: 0.0253\n",
      "Epoch 85/200\n",
      "453/453 [==============================] - 22s 49ms/step - loss: 0.0252 - val_loss: 0.0253\n",
      "Epoch 86/200\n",
      "453/453 [==============================] - 22s 49ms/step - loss: 0.0252 - val_loss: 0.0253\n",
      "Epoch 87/200\n",
      "453/453 [==============================] - 27s 60ms/step - loss: 0.0252 - val_loss: 0.0255\n",
      "Epoch 90/200\n",
      "453/453 [==============================] - 25s 54ms/step - loss: 0.0253 - val_loss: 0.0255\n",
      "Epoch 91/200\n",
      "453/453 [==============================] - 24s 53ms/step - loss: 0.0251 - val_loss: 0.0252\n",
      "Epoch 92/200\n",
      "453/453 [==============================] - 25s 55ms/step - loss: 0.0251 - val_loss: 0.0252\n",
      "Epoch 93/200\n",
      "453/453 [==============================] - 25s 55ms/step - loss: 0.0251 - val_loss: 0.0253\n",
      "Epoch 94/200\n",
      "453/453 [==============================] - 22s 49ms/step - loss: 0.0246 - val_loss: 0.0247\n",
      "Epoch 132/200\n",
      "453/453 [==============================] - 23s 50ms/step - loss: 0.0246 - val_loss: 0.0249\n",
      "Epoch 133/200\n",
      "453/453 [==============================] - 24s 54ms/step - loss: 0.0246 - val_loss: 0.0247\n",
      "Epoch 134/200\n",
      "453/453 [==============================] - 22s 49ms/step - loss: 0.0246 - val_loss: 0.0247\n",
      "Epoch 135/200\n",
      "453/453 [==============================] - 22s 49ms/step - loss: 0.0245 - val_loss: 0.0249\n",
      "Epoch 136/200\n",
      "453/453 [==============================] - 26s 56ms/step - loss: 0.0246 - val_loss: 0.0247\n",
      "Epoch 137/200\n",
      "453/453 [==============================] - 23s 52ms/step - loss: 0.0245 - val_loss: 0.0246\n",
      "Epoch 138/200\n",
      "453/453 [==============================] - 24s 53ms/step - loss: 0.0245 - val_loss: 0.0246\n",
      "Epoch 139/200\n",
      "453/453 [==============================] - 22s 48ms/step - loss: 0.0245 - val_loss: 0.0247\n",
      "Epoch 140/200\n",
      "453/453 [==============================] - 21s 47ms/step - loss: 0.0245 - val_loss: 0.0246\n",
      "Epoch 141/200\n",
      "453/453 [==============================] - 24s 53ms/step - loss: 0.0246 - val_loss: 0.0247\n",
      "Epoch 142/200\n",
      "453/453 [==============================] - 23s 51ms/step - loss: 0.0245 - val_loss: 0.0246\n",
      "Epoch 143/200\n",
      "453/453 [==============================] - 22s 47ms/step - loss: 0.0244 - val_loss: 0.0246\n",
      "Epoch 144/200\n",
      "453/453 [==============================] - 22s 48ms/step - loss: 0.0246 - val_loss: 0.0246\n",
      "Epoch 145/200\n",
      "453/453 [==============================] - 25s 56ms/step - loss: 0.0245 - val_loss: 0.0246\n",
      "Epoch 146/200\n",
      "453/453 [==============================] - 25s 56ms/step - loss: 0.0245 - val_loss: 0.0245\n",
      "Epoch 147/200\n",
      "453/453 [==============================] - 23s 51ms/step - loss: 0.0244 - val_loss: 0.0246\n",
      "Epoch 148/200\n",
      "453/453 [==============================] - 22s 50ms/step - loss: 0.0244 - val_loss: 0.0245\n",
      "Epoch 149/200\n",
      "453/453 [==============================] - 24s 53ms/step - loss: 0.0244 - val_loss: 0.0245\n",
      "Epoch 150/200\n",
      "453/453 [==============================] - 26s 58ms/step - loss: 0.0244 - val_loss: 0.0246\n",
      "Epoch 151/200\n",
      "453/453 [==============================] - 25s 55ms/step - loss: 0.0245 - val_loss: 0.0247\n",
      "Epoch 155/200\n",
      "453/453 [==============================] - 22s 49ms/step - loss: 0.0244 - val_loss: 0.0245\n",
      "Epoch 156/200\n",
      "453/453 [==============================] - 28s 61ms/step - loss: 0.0243 - val_loss: 0.0246\n",
      "Epoch 157/200\n",
      "453/453 [==============================] - 26s 58ms/step - loss: 0.0244 - val_loss: 0.0245\n",
      "Epoch 158/200\n",
      "453/453 [==============================] - 25s 56ms/step - loss: 0.0243 - val_loss: 0.0244\n",
      "Epoch 159/200\n",
      "453/453 [==============================] - 24s 54ms/step - loss: 0.0243 - val_loss: 0.0245\n",
      "Epoch 160/200\n",
      "453/453 [==============================] - 24s 52ms/step - loss: 0.0243 - val_loss: 0.0245\n",
      "Epoch 161/200\n",
      "453/453 [==============================] - 23s 52ms/step - loss: 0.0244 - val_loss: 0.0247\n",
      "Epoch 162/200\n",
      "453/453 [==============================] - 23s 52ms/step - loss: 0.0243 - val_loss: 0.0244\n",
      "Epoch 163/200\n",
      "453/453 [==============================] - 23s 51ms/step - loss: 0.0243 - val_loss: 0.0244\n",
      "Epoch 164/200\n",
      "453/453 [==============================] - 22s 48ms/step - loss: 0.0243 - val_loss: 0.0244\n",
      "Epoch 165/200\n",
      "453/453 [==============================] - 23s 51ms/step - loss: 0.0243 - val_loss: 0.0244\n",
      "Epoch 166/200\n",
      "453/453 [==============================] - 23s 52ms/step - loss: 0.0243 - val_loss: 0.0244\n",
      "Epoch 167/200\n",
      "453/453 [==============================] - 22s 49ms/step - loss: 0.0244 - val_loss: 0.0244\n",
      "Epoch 168/200\n",
      "453/453 [==============================] - 22s 49ms/step - loss: 0.0244 - val_loss: 0.0244\n",
      "Epoch 169/200\n",
      "453/453 [==============================] - 22s 48ms/step - loss: 0.0243 - val_loss: 0.0244\n",
      "Epoch 170/200\n",
      "453/453 [==============================] - 23s 51ms/step - loss: 0.0242 - val_loss: 0.0243\n",
      "Epoch 171/200\n",
      "453/453 [==============================] - 21s 47ms/step - loss: 0.0243 - val_loss: 0.0244\n",
      "Epoch 172/200\n",
      "453/453 [==============================] - 21s 47ms/step - loss: 0.0243 - val_loss: 0.0243\n",
      "Epoch 173/200\n",
      "453/453 [==============================] - 21s 47ms/step - loss: 0.0242 - val_loss: 0.0244\n",
      "Epoch 174/200\n",
      "453/453 [==============================] - 21s 46ms/step - loss: 0.0243 - val_loss: 0.0243\n",
      "Epoch 175/200\n",
      "453/453 [==============================] - 21s 46ms/step - loss: 0.0242 - val_loss: 0.0244\n",
      "Epoch 176/200\n",
      "453/453 [==============================] - 21s 47ms/step - loss: 0.0242 - val_loss: 0.0243\n",
      "Epoch 177/200\n",
      "453/453 [==============================] - 26s 57ms/step - loss: 0.0242 - val_loss: 0.0243\n",
      "Epoch 178/200\n",
      "453/453 [==============================] - 23s 52ms/step - loss: 0.0242 - val_loss: 0.0245\n",
      "Epoch 182/200\n",
      "453/453 [==============================] - 25s 54ms/step - loss: 0.0242 - val_loss: 0.0243\n",
      "Epoch 183/200\n",
      "453/453 [==============================] - 23s 50ms/step - loss: 0.0242 - val_loss: 0.0243\n",
      "Epoch 184/200\n",
      "453/453 [==============================] - 21s 47ms/step - loss: 0.0242 - val_loss: 0.0244\n",
      "Epoch 185/200\n",
      "453/453 [==============================] - 21s 47ms/step - loss: 0.0242 - val_loss: 0.0243\n",
      "Epoch 186/200\n",
      "453/453 [==============================] - 21s 47ms/step - loss: 0.0242 - val_loss: 0.0244\n",
      "Epoch 187/200\n",
      "453/453 [==============================] - 21s 46ms/step - loss: 0.0242 - val_loss: 0.0244\n",
      "Epoch 188/200\n",
      "453/453 [==============================] - 21s 46ms/step - loss: 0.0242 - val_loss: 0.0243\n",
      "Epoch 189/200\n",
      "453/453 [==============================] - 21s 46ms/step - loss: 0.0241 - val_loss: 0.0243\n",
      "Epoch 190/200\n",
      "453/453 [==============================] - 21s 46ms/step - loss: 0.0241 - val_loss: 0.0243\n",
      "Epoch 191/200\n",
      "453/453 [==============================] - 21s 47ms/step - loss: 0.0241 - val_loss: 0.0242\n",
      "Epoch 192/200\n",
      "453/453 [==============================] - 21s 46ms/step - loss: 0.0241 - val_loss: 0.0242\n",
      "Epoch 193/200\n",
      "453/453 [==============================] - 21s 47ms/step - loss: 0.0241 - val_loss: 0.0242\n",
      "Epoch 194/200\n",
      "453/453 [==============================] - 21s 47ms/step - loss: 0.0241 - val_loss: 0.0243\n",
      "Epoch 195/200\n",
      "453/453 [==============================] - 22s 48ms/step - loss: 0.0241 - val_loss: 0.0242\n",
      "Epoch 196/200\n",
      "453/453 [==============================] - 21s 47ms/step - loss: 0.0241 - val_loss: 0.0242\n",
      "Epoch 197/200\n",
      "453/453 [==============================] - 22s 48ms/step - loss: 0.0241 - val_loss: 0.0242\n",
      "Epoch 198/200\n",
      "453/453 [==============================] - 21s 47ms/step - loss: 0.0241 - val_loss: 0.0242\n",
      "Epoch 199/200\n",
      "453/453 [==============================] - 21s 47ms/step - loss: 0.0241 - val_loss: 0.0242\n",
      "Epoch 200/200\n",
      "453/453 [==============================] - 21s 47ms/step - loss: 0.0241 - val_loss: 0.0242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25589dee828>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = prepare_data(NUM_CHANNEL)\n",
    "\n",
    "x = x / 255.\n",
    "y = y / 255.\n",
    "\n",
    "x = x.reshape((-1, HEIGHT, WIDTH, NUM_CHANNEL))\n",
    "y = y.reshape((-1, HEIGHT, WIDTH, NUM_CHANNEL))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=200,\n",
    "                batch_size=20,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs = autoencoder.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 1\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# display original\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "plt.imshow(y_test[idx].reshape(HEIGHT, WIDTH))\n",
    "plt.gray()\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "\n",
    "# display reconstruction\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "plt.imshow(decoded_imgs[idx].reshape(HEIGHT, WIDTH))\n",
    "plt.gray()\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
