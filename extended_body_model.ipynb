{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Lambda, Reshape, Flatten, ZeroPadding2D, Layer\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from data_generator import DataGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data/data_simple_movement/cartesian/'\n",
    "HEIGHT = 100\n",
    "WIDTH = 100\n",
    "NUM_CHANNEL = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMCLayer(Layer):\n",
    "    \"\"\"Custom MMC layer, perfoms single step(iteration) given input\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(MMCLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                     shape=(input_shape[1], self.output_dim),\n",
    "                                     initializer='uniform',\n",
    "                                     trainable=True)\n",
    "        super(MMCLayer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        df = 1 / 5\n",
    "        w = np.array([\n",
    "            [1, df, 0], [-1, 0, 1], [0, 0, 1]\n",
    "        ])\n",
    "        # Can not set mmc target \n",
    "        return K.dot(K.dot(x, self.kernel), K.constant(w))\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100, 100, 2)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 100, 100, 16)      304       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 50, 50, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 50, 50, 8)         1160      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 25, 25, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 25, 25, 8)         584       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 8)         0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 1352)              0         \n",
      "_________________________________________________________________\n",
      "mmc_layer_1 (MMCLayer)       (None, 3)                 4056      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1352)              5408      \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 13, 13, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 13, 13, 8)         584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 26, 26, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 26, 26, 8)         584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 52, 52, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 50, 50, 16)        1168      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 100, 100, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 100, 100, 1)       145       \n",
      "=================================================================\n",
      "Total params: 13,993\n",
      "Trainable params: 13,993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_img = Input(shape=(HEIGHT, WIDTH, NUM_CHANNEL))\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "x = Reshape((13*13*8,))(x)\n",
    "x = MMCLayer(3)(x)\n",
    "x = Dense(13*13*8, activation='relu')(x)\n",
    "x = Reshape((13, 13, 8))(x)\n",
    "\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 86 samples, validate on 10 samples\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 4s 41ms/step - loss: 0.6863 - val_loss: 0.6638\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 3s 29ms/step - loss: 0.5465 - val_loss: 0.2361\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 3s 29ms/step - loss: 0.2304 - val_loss: 0.2074\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 3s 32ms/step - loss: 0.1785 - val_loss: 0.1792\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 3s 38ms/step - loss: 0.1595 - val_loss: 0.1525\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.1453 - val_loss: 0.1359\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 3s 39ms/step - loss: 0.1322 - val_loss: 0.1257\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.1221 - val_loss: 0.1168\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.1127 - val_loss: 0.1079\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.1036 - val_loss: 0.0988\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0945 - val_loss: 0.0898\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0857 - val_loss: 0.0812\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.0772 - val_loss: 0.0727\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0688 - val_loss: 0.0644\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 3s 36ms/step - loss: 0.0605 - val_loss: 0.0563\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 3s 36ms/step - loss: 0.0526 - val_loss: 0.0487\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 3s 39ms/step - loss: 0.0455 - val_loss: 0.0424\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 3s 39ms/step - loss: 0.0402 - val_loss: 0.0376\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 3s 39ms/step - loss: 0.0356 - val_loss: 0.0332\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 3s 36ms/step - loss: 0.0315 - val_loss: 0.0300\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0284 - val_loss: 0.0275\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0256 - val_loss: 0.0249\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0239 - val_loss: 0.0238\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0230 - val_loss: 0.0234\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 3s 38ms/step - loss: 0.0226 - val_loss: 0.0229\n",
      "Epoch 26/100\n",
      "86/86 [==============================] - 3s 38ms/step - loss: 0.0223 - val_loss: 0.0226\n",
      "Epoch 27/100\n",
      "86/86 [==============================] - 3s 36ms/step - loss: 0.0221 - val_loss: 0.0224\n",
      "Epoch 28/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0220 - val_loss: 0.0224\n",
      "Epoch 29/100\n",
      "86/86 [==============================] - 3s 38ms/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 30/100\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0217 - val_loss: 0.0221\n",
      "Epoch 31/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0217 - val_loss: 0.0221\n",
      "Epoch 32/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0216 - val_loss: 0.0220\n",
      "Epoch 33/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0216 - val_loss: 0.0220\n",
      "Epoch 34/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0215 - val_loss: 0.0218\n",
      "Epoch 35/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0214 - val_loss: 0.0219\n",
      "Epoch 36/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0215 - val_loss: 0.0218\n",
      "Epoch 37/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0213 - val_loss: 0.0217\n",
      "Epoch 38/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0213 - val_loss: 0.0216\n",
      "Epoch 39/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0213 - val_loss: 0.0217\n",
      "Epoch 40/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0212 - val_loss: 0.0217\n",
      "Epoch 41/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0212 - val_loss: 0.0215\n",
      "Epoch 42/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0212 - val_loss: 0.0215\n",
      "Epoch 43/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0211 - val_loss: 0.0215\n",
      "Epoch 44/100\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0211 - val_loss: 0.0216\n",
      "Epoch 45/100\n",
      "86/86 [==============================] - 4s 43ms/step - loss: 0.0211 - val_loss: 0.0215\n",
      "Epoch 46/100\n",
      "86/86 [==============================] - 4s 42ms/step - loss: 0.0211 - val_loss: 0.0214\n",
      "Epoch 47/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0211 - val_loss: 0.0215\n",
      "Epoch 48/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0211 - val_loss: 0.0215\n",
      "Epoch 49/100\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.0211 - val_loss: 0.0214\n",
      "Epoch 50/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0210 - val_loss: 0.0214\n",
      "Epoch 51/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0210 - val_loss: 0.0215\n",
      "Epoch 52/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0210 - val_loss: 0.0214\n",
      "Epoch 53/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0210 - val_loss: 0.0214\n",
      "Epoch 54/100\n",
      "86/86 [==============================] - 3s 32ms/step - loss: 0.0210 - val_loss: 0.0214\n",
      "Epoch 55/100\n",
      "86/86 [==============================] - 3s 32ms/step - loss: 0.0210 - val_loss: 0.0215\n",
      "Epoch 56/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0210 - val_loss: 0.0214\n",
      "Epoch 57/100\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.0209 - val_loss: 0.0214\n",
      "Epoch 58/100\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0210 - val_loss: 0.0212\n",
      "Epoch 59/100\n",
      "86/86 [==============================] - 3s 36ms/step - loss: 0.0210 - val_loss: 0.0216\n",
      "Epoch 60/100\n",
      "86/86 [==============================] - 3s 36ms/step - loss: 0.0210 - val_loss: 0.0213\n",
      "Epoch 61/100\n",
      "86/86 [==============================] - 3s 36ms/step - loss: 0.0210 - val_loss: 0.0213\n",
      "Epoch 62/100\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0210 - val_loss: 0.0212\n",
      "Epoch 63/100\n",
      "86/86 [==============================] - 3s 38ms/step - loss: 0.0209 - val_loss: 0.0215\n",
      "Epoch 64/100\n",
      "86/86 [==============================] - 3s 39ms/step - loss: 0.0210 - val_loss: 0.0213\n",
      "Epoch 65/100\n",
      "86/86 [==============================] - 3s 39ms/step - loss: 0.0210 - val_loss: 0.0213\n",
      "Epoch 66/100\n",
      "86/86 [==============================] - 3s 36ms/step - loss: 0.0209 - val_loss: 0.0213\n",
      "Epoch 67/100\n",
      "86/86 [==============================] - 3s 39ms/step - loss: 0.0209 - val_loss: 0.0212\n",
      "Epoch 68/100\n",
      "86/86 [==============================] - 3s 38ms/step - loss: 0.0209 - val_loss: 0.0213\n",
      "Epoch 69/100\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0209 - val_loss: 0.0212\n",
      "Epoch 70/100\n",
      "86/86 [==============================] - 3s 38ms/step - loss: 0.0209 - val_loss: 0.0213\n",
      "Epoch 71/100\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0209 - val_loss: 0.0212\n",
      "Epoch 72/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0209 - val_loss: 0.0212\n",
      "Epoch 73/100\n",
      "86/86 [==============================] - 3s 36ms/step - loss: 0.0209 - val_loss: 0.0213\n",
      "Epoch 74/100\n",
      "86/86 [==============================] - 3s 36ms/step - loss: 0.0208 - val_loss: 0.0213\n",
      "Epoch 75/100\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0209 - val_loss: 0.0212\n",
      "Epoch 76/100\n",
      "86/86 [==============================] - 3s 36ms/step - loss: 0.0209 - val_loss: 0.0211\n",
      "Epoch 77/100\n",
      "86/86 [==============================] - 3s 36ms/step - loss: 0.0208 - val_loss: 0.0213\n",
      "Epoch 78/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0208 - val_loss: 0.0212\n",
      "Epoch 79/100\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.0208 - val_loss: 0.0212\n",
      "Epoch 80/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0208 - val_loss: 0.0212\n",
      "Epoch 81/100\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0209 - val_loss: 0.0212\n",
      "Epoch 82/100\n",
      "86/86 [==============================] - 3s 39ms/step - loss: 0.0209 - val_loss: 0.0212\n",
      "Epoch 83/100\n",
      "86/86 [==============================] - 5s 54ms/step - loss: 0.0208 - val_loss: 0.0212\n",
      "Epoch 84/100\n",
      "86/86 [==============================] - 5s 55ms/step - loss: 0.0208 - val_loss: 0.0211\n",
      "Epoch 85/100\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0208 - val_loss: 0.0212\n",
      "Epoch 86/100\n",
      "86/86 [==============================] - 3s 38ms/step - loss: 0.0208 - val_loss: 0.0211\n",
      "Epoch 87/100\n",
      "86/86 [==============================] - 3s 36ms/step - loss: 0.0208 - val_loss: 0.0213\n",
      "Epoch 88/100\n",
      "86/86 [==============================] - 3s 39ms/step - loss: 0.0208 - val_loss: 0.0211\n",
      "Epoch 89/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0209 - val_loss: 0.0212\n",
      "Epoch 90/100\n",
      "86/86 [==============================] - 3s 36ms/step - loss: 0.0209 - val_loss: 0.0212\n",
      "Epoch 91/100\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.0208 - val_loss: 0.0211\n",
      "Epoch 92/100\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.0208 - val_loss: 0.0211\n",
      "Epoch 93/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0208 - val_loss: 0.0212\n",
      "Epoch 94/100\n",
      "86/86 [==============================] - 3s 36ms/step - loss: 0.0208 - val_loss: 0.0211\n",
      "Epoch 95/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0208 - val_loss: 0.0212\n",
      "Epoch 96/100\n",
      "86/86 [==============================] - 3s 36ms/step - loss: 0.0208 - val_loss: 0.0212\n",
      "Epoch 97/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0208 - val_loss: 0.0211\n",
      "Epoch 98/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0207 - val_loss: 0.0211\n",
      "Epoch 99/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0207 - val_loss: 0.0211\n",
      "Epoch 100/100\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.0208 - val_loss: 0.0211\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26be9c9ae80>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgen = DataGen(HEIGHT, WIDTH, NUM_CHANNEL)\n",
    "x, y = dgen.get_data(path=os.path.abspath(DATA_DIR),\n",
    "                     target_mmc_out=False,\n",
    "                     size=8,\n",
    "                     channel_first=False)\n",
    "\n",
    "x = x / 255.\n",
    "y = y / 255.\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, shuffle=True)\n",
    "\n",
    "autoencoder.fit(x_train, y_train,\n",
    "                epochs=100,\n",
    "                batch_size=10,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs = autoencoder.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 6  # change index for differnet test image\n",
    "\n",
    "fig, axes = plt.subplots(figsize=((10, 3)), nrows=1, ncols=NUM_CHANNEL+2)\n",
    "\n",
    "# display input images\n",
    "for i, ax in enumerate(axes.flat[:NUM_CHANNEL]):\n",
    "    ax.imshow(x_test[idx][:, :, i].reshape(HEIGHT, WIDTH), cmap='gray')\n",
    "    ax.axis('off'), ax.set_title('step ' + str(i+1))\n",
    "    \n",
    "# display prediction\n",
    "axes[NUM_CHANNEL].imshow(decoded_imgs[idx][:, :, 0].reshape(HEIGHT, WIDTH), cmap='gray')\n",
    "axes[NUM_CHANNEL].axis('off'), axes[NUM_CHANNEL].set_title('predicted')\n",
    "\n",
    "# display target\n",
    "axes[NUM_CHANNEL+1].imshow(y_test[idx][:, :, 0].reshape(HEIGHT, WIDTH), cmap='gray')\n",
    "axes[NUM_CHANNEL+1].axis('off'), axes[NUM_CHANNEL+1].set_title('target')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
