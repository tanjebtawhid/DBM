{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from data_generator import DataGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMC(nn.Module):\n",
    "    \n",
    "    def __init__(self, beta=5, num_iter=20):\n",
    "        super(MMC, self).__init__()\n",
    "        self.df = 1. / beta\n",
    "        self.num_iter = num_iter\n",
    "        # self.weights = nn.Parameter(data= torch.tensor([[1, self.df, 0], [-1, 0, 1], [0, 0, 1]]),\n",
    "        #                            requires_grad=False)\n",
    "        self.weights = nn.Parameter(data=torch.from_numpy(self._init_weights()).float(), \n",
    "                                    requires_grad=False)\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initializes and broadcasts weigth matrix for batch processing\"\"\"\n",
    "        w = np.array([[1, self.df, 0], [-1, 0, 1], [0, 0, 1]])\n",
    "        return np.broadcast_to(w, (9, 3, 3))  # 9 refers to batch size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.bmm(torch.pow(self.weights, self.num_iter), x.unsqueeze(2)).squeeze(2)  # num_iter steps\n",
    "        # x = torch.matmul(self.weights, x.transpose_(0, 1))  # single step\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_channel):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=num_channel, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(8, 8, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(8, 8, 2)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(200, 128)\n",
    "        self.fc2 = nn.Linear(128, 3)\n",
    "        self.mmc = MMC()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = x.view(-1, 5*5*8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        # print('before: ', x)\n",
    "        x = self.mmc(x)\n",
    "        # print('after: ', x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([16, 1, 3, 3])\n",
      "conv1.bias torch.Size([16])\n",
      "conv2.weight torch.Size([8, 16, 3, 3])\n",
      "conv2.bias torch.Size([8])\n",
      "conv3.weight torch.Size([8, 8, 3, 3])\n",
      "conv3.bias torch.Size([8])\n",
      "conv4.weight torch.Size([8, 8, 2, 2])\n",
      "conv4.bias torch.Size([8])\n",
      "fc1.weight torch.Size([128, 200])\n",
      "fc1.bias torch.Size([128])\n",
      "fc2.weight torch.Size([3, 128])\n",
      "fc2.bias torch.Size([3])\n",
      "mmc.weights torch.Size([9, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "model = Encoder(1)\n",
    "for n, p in model.named_parameters():\n",
    "    print(n, p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(model, (1, 100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_channel):\n",
    "    net = Encoder(num_channel=num_channel)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "    \n",
    "    return net, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(x):\n",
    "    return torch.from_numpy(x).float()\n",
    "\n",
    "def get_data(height, width, num_channel, path, size, bs):\n",
    "    dgen = DataGen(height, width, num_channel)\n",
    "    x, y = dgen.get_data(path, True, size, True)\n",
    "    print('Data loaded...\\nx:{}\\ty:{}\\n'.format(x.shape, y.shape))\n",
    "    \n",
    "    x = x / 255.\n",
    "    x = x[:90]\n",
    "    y = y[:90]\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, shuffle=True)\n",
    "    x_train, x_test, y_train, y_test = map(to_tensor, (x_train, x_test, y_train, y_test))\n",
    "    \n",
    "    train_ds = TensorDataset(x_train, y_train)\n",
    "    train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "\n",
    "    test_ds = TensorDataset(x_test, y_test)\n",
    "    test_dl = DataLoader(test_ds, batch_size=bs)\n",
    "    \n",
    "    return train_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_flow(named_parameters):\n",
    "    \"\"\"To check gradient flow through the network\"\"\"\n",
    "    ave_grads = []\n",
    "    max_grads= []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean())\n",
    "            max_grads.append(p.grad.abs().max())\n",
    "    print('layers: {}'.format(layers))\n",
    "    print('max: {}'.format(max_grads))\n",
    "    print('mean: {}\\n\\n'.format( ave_grads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(net, optimizer, train_dl, test_dl, epochs):\n",
    "    loss_function = nn.MSELoss()\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        for x, y in train_dl:\n",
    "            pred = net(x)\n",
    "            loss = loss_function(pred, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # grad_flow(net.named_parameters())\n",
    "            optimizer.step()\n",
    "\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            test_loss = sum(loss_function(net(x), y) for x, y in test_dl)\n",
    "        print('epoch: {}/{} - train_loss: {:.4f} - test_loss: {:.4f}'.format(epoch + 1, epochs, loss, test_loss / len(test_dl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data/data_simple_movement/cartesian/'\n",
    "HEIGHT = 100\n",
    "WIDTH = 100\n",
    "NUM_CHANNEL = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded...\n",
      "x:(96, 2, 100, 100)\ty:(96, 3)\n",
      "\n",
      "epoch: 1/100 - train_loss: 0.3751 - test_loss: 0.3592\n",
      "epoch: 2/100 - train_loss: 0.3616 - test_loss: 0.3587\n",
      "epoch: 3/100 - train_loss: 0.3330 - test_loss: 0.3648\n",
      "epoch: 4/100 - train_loss: 0.3379 - test_loss: 0.3661\n",
      "epoch: 5/100 - train_loss: 0.1527 - test_loss: 0.3632\n",
      "epoch: 6/100 - train_loss: 0.4553 - test_loss: 0.3604\n",
      "epoch: 7/100 - train_loss: 0.2712 - test_loss: 0.3595\n",
      "epoch: 8/100 - train_loss: 0.2686 - test_loss: 0.3608\n",
      "epoch: 9/100 - train_loss: 0.1943 - test_loss: 0.3605\n",
      "epoch: 10/100 - train_loss: 0.2480 - test_loss: 0.3618\n",
      "epoch: 11/100 - train_loss: 0.3643 - test_loss: 0.3643\n",
      "epoch: 12/100 - train_loss: 0.3403 - test_loss: 0.3622\n",
      "epoch: 13/100 - train_loss: 0.2190 - test_loss: 0.3607\n",
      "epoch: 14/100 - train_loss: 0.2686 - test_loss: 0.3600\n",
      "epoch: 15/100 - train_loss: 0.3098 - test_loss: 0.3599\n",
      "epoch: 16/100 - train_loss: 0.2635 - test_loss: 0.3604\n",
      "epoch: 17/100 - train_loss: 0.4924 - test_loss: 0.3629\n",
      "epoch: 18/100 - train_loss: 0.2698 - test_loss: 0.3616\n",
      "epoch: 19/100 - train_loss: 0.2956 - test_loss: 0.3617\n",
      "epoch: 20/100 - train_loss: 0.4036 - test_loss: 0.3609\n",
      "epoch: 21/100 - train_loss: 0.3194 - test_loss: 0.3604\n",
      "epoch: 22/100 - train_loss: 0.4593 - test_loss: 0.3612\n",
      "epoch: 23/100 - train_loss: 0.1762 - test_loss: 0.3614\n",
      "epoch: 24/100 - train_loss: 0.4384 - test_loss: 0.3613\n",
      "epoch: 25/100 - train_loss: 0.1788 - test_loss: 0.3616\n",
      "epoch: 26/100 - train_loss: 0.3857 - test_loss: 0.3610\n",
      "epoch: 27/100 - train_loss: 0.2576 - test_loss: 0.3601\n",
      "epoch: 28/100 - train_loss: 0.2816 - test_loss: 0.3603\n",
      "epoch: 29/100 - train_loss: 0.3122 - test_loss: 0.3608\n",
      "epoch: 30/100 - train_loss: 0.4240 - test_loss: 0.3609\n",
      "epoch: 31/100 - train_loss: 0.2462 - test_loss: 0.3594\n",
      "epoch: 32/100 - train_loss: 0.2475 - test_loss: 0.3606\n",
      "epoch: 33/100 - train_loss: 0.3550 - test_loss: 0.3594\n",
      "epoch: 34/100 - train_loss: 0.3605 - test_loss: 0.3597\n",
      "epoch: 35/100 - train_loss: 0.2546 - test_loss: 0.3592\n",
      "epoch: 36/100 - train_loss: 0.4370 - test_loss: 0.3575\n",
      "epoch: 37/100 - train_loss: 0.2201 - test_loss: 0.3482\n",
      "epoch: 38/100 - train_loss: 0.2830 - test_loss: 0.3300\n",
      "epoch: 39/100 - train_loss: 0.2163 - test_loss: 0.3072\n",
      "epoch: 40/100 - train_loss: 0.2657 - test_loss: 0.3053\n",
      "epoch: 41/100 - train_loss: 0.2568 - test_loss: 0.3005\n",
      "epoch: 42/100 - train_loss: 0.1692 - test_loss: 0.2983\n",
      "epoch: 43/100 - train_loss: 0.2872 - test_loss: 0.2961\n",
      "epoch: 44/100 - train_loss: 0.1869 - test_loss: 0.2865\n",
      "epoch: 45/100 - train_loss: 0.2958 - test_loss: 0.2840\n",
      "epoch: 46/100 - train_loss: 0.2482 - test_loss: 0.2913\n",
      "epoch: 47/100 - train_loss: 0.2663 - test_loss: 0.2778\n",
      "epoch: 48/100 - train_loss: 0.3177 - test_loss: 0.2786\n",
      "epoch: 49/100 - train_loss: 0.2509 - test_loss: 0.2774\n",
      "epoch: 50/100 - train_loss: 0.2782 - test_loss: 0.2970\n",
      "epoch: 51/100 - train_loss: 0.1927 - test_loss: 0.2801\n",
      "epoch: 52/100 - train_loss: 0.3075 - test_loss: 0.2779\n",
      "epoch: 53/100 - train_loss: 0.3152 - test_loss: 0.2773\n",
      "epoch: 54/100 - train_loss: 0.1542 - test_loss: 0.2767\n",
      "epoch: 55/100 - train_loss: 0.2610 - test_loss: 0.2759\n",
      "epoch: 56/100 - train_loss: 0.1524 - test_loss: 0.2769\n",
      "epoch: 57/100 - train_loss: 0.2415 - test_loss: 0.2764\n",
      "epoch: 58/100 - train_loss: 0.1759 - test_loss: 0.2766\n",
      "epoch: 59/100 - train_loss: 0.2447 - test_loss: 0.2763\n",
      "epoch: 60/100 - train_loss: 0.1814 - test_loss: 0.2762\n",
      "epoch: 61/100 - train_loss: 0.2113 - test_loss: 0.2788\n",
      "epoch: 62/100 - train_loss: 0.2059 - test_loss: 0.2768\n",
      "epoch: 63/100 - train_loss: 0.2600 - test_loss: 0.2814\n",
      "epoch: 64/100 - train_loss: 0.1820 - test_loss: 0.2765\n",
      "epoch: 65/100 - train_loss: 0.3045 - test_loss: 0.2768\n",
      "epoch: 66/100 - train_loss: 0.1503 - test_loss: 0.2778\n",
      "epoch: 67/100 - train_loss: 0.2215 - test_loss: 0.2760\n",
      "epoch: 68/100 - train_loss: 0.2383 - test_loss: 0.2761\n",
      "epoch: 69/100 - train_loss: 0.2677 - test_loss: 0.2764\n",
      "epoch: 70/100 - train_loss: 0.1567 - test_loss: 0.2757\n",
      "epoch: 71/100 - train_loss: 0.3121 - test_loss: 0.2794\n",
      "epoch: 72/100 - train_loss: 0.2183 - test_loss: 0.2770\n",
      "epoch: 73/100 - train_loss: 0.1850 - test_loss: 0.2785\n",
      "epoch: 74/100 - train_loss: 0.1510 - test_loss: 0.2774\n",
      "epoch: 75/100 - train_loss: 0.2152 - test_loss: 0.2760\n",
      "epoch: 76/100 - train_loss: 0.2971 - test_loss: 0.2785\n",
      "epoch: 77/100 - train_loss: 0.2814 - test_loss: 0.2757\n",
      "epoch: 78/100 - train_loss: 0.2185 - test_loss: 0.2758\n",
      "epoch: 79/100 - train_loss: 0.1248 - test_loss: 0.2767\n",
      "epoch: 80/100 - train_loss: 0.2303 - test_loss: 0.2789\n",
      "epoch: 81/100 - train_loss: 0.2201 - test_loss: 0.2770\n",
      "epoch: 82/100 - train_loss: 0.3383 - test_loss: 0.2761\n",
      "epoch: 83/100 - train_loss: 0.2490 - test_loss: 0.2767\n",
      "epoch: 84/100 - train_loss: 0.1243 - test_loss: 0.2768\n",
      "epoch: 85/100 - train_loss: 0.2534 - test_loss: 0.2758\n",
      "epoch: 86/100 - train_loss: 0.1863 - test_loss: 0.2827\n",
      "epoch: 87/100 - train_loss: 0.2380 - test_loss: 0.2765\n",
      "epoch: 88/100 - train_loss: 0.1350 - test_loss: 0.2783\n",
      "epoch: 89/100 - train_loss: 0.2383 - test_loss: 0.2763\n",
      "epoch: 90/100 - train_loss: 0.2035 - test_loss: 0.2770\n",
      "epoch: 91/100 - train_loss: 0.1898 - test_loss: 0.2761\n",
      "epoch: 92/100 - train_loss: 0.1495 - test_loss: 0.2764\n",
      "epoch: 93/100 - train_loss: 0.2344 - test_loss: 0.2788\n",
      "epoch: 94/100 - train_loss: 0.1846 - test_loss: 0.2783\n",
      "epoch: 95/100 - train_loss: 0.2359 - test_loss: 0.2761\n",
      "epoch: 96/100 - train_loss: 0.3469 - test_loss: 0.2768\n",
      "epoch: 97/100 - train_loss: 0.2614 - test_loss: 0.2760\n",
      "epoch: 98/100 - train_loss: 0.1627 - test_loss: 0.2795\n",
      "epoch: 99/100 - train_loss: 0.2458 - test_loss: 0.2763\n",
      "epoch: 100/100 - train_loss: 0.1656 - test_loss: 0.2767\n"
     ]
    }
   ],
   "source": [
    "net, optimizer = get_model(num_channel=2)\n",
    "\n",
    "train_dl, test_dl = get_data(\n",
    "    height=HEIGHT, \n",
    "    width=WIDTH,\n",
    "    num_channel=NUM_CHANNEL,\n",
    "    path=DATA_DIR,\n",
    "    size=8,\n",
    "    bs=9\n",
    ")\n",
    "\n",
    "fit(net, optimizer, train_dl, test_dl, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: tensor([[ 0.1591,  0.3187,  0.1597],\n",
      "        [ 0.2370,  0.4683,  0.2313],\n",
      "        [ 0.1441,  0.2766,  0.1325],\n",
      "        [-0.1475, -0.2922, -0.1447],\n",
      "        [-0.0142, -0.0353, -0.0212],\n",
      "        [-0.2521, -0.5059, -0.2539],\n",
      "        [-0.0142, -0.0353, -0.0212],\n",
      "        [ 0.2904,  0.5822,  0.2918],\n",
      "        [-0.2585, -0.5175, -0.2590]], grad_fn=<SqueezeBackward1>),\n",
      "target: tensor([[ 5.0227e-01,  3.6272e-03,  5.0000e-01],\n",
      "        [ 8.6668e-01,  1.8136e-03,  8.6603e-01],\n",
      "        [ 5.0227e-01,  3.6272e-03,  5.0000e-01],\n",
      "        [-4.9545e-01,  7.2543e-03, -5.0000e-01],\n",
      "        [ 7.0850e-01,  2.7204e-03,  7.0711e-01],\n",
      "        [-7.0292e-01,  8.1611e-03, -7.0711e-01],\n",
      "        [-9.6403e-01,  9.9747e-03, -9.6593e-01],\n",
      "        [ 9.6610e-01,  9.0679e-04,  9.6593e-01],\n",
      "        [-7.0292e-01,  8.1611e-03, -7.0711e-01]])\n"
     ]
    }
   ],
   "source": [
    "for x, y in test_dl:\n",
    "    print('prediction: {},\\ntarget: {}'.format(net(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
