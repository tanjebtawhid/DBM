{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from data_generator import DataGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMC(nn.Module):\n",
    "    \n",
    "    def __init__(self, beta=5, num_iter=20):\n",
    "        super(MMC, self).__init__()\n",
    "        self.df = 1. / beta\n",
    "        self.num_iter = num_iter\n",
    "        self.weights = nn.Parameter(data= torch.tensor([[1, self.df, 0], [-1, 0, 1], [0, 0, 1]]),\n",
    "                                    requires_grad=False)\n",
    "        # self.weights = nn.Parameter(data=torch.from_numpy(self._init_weights()).float(), \n",
    "        #                             requires_grad=False)\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        w = np.array([[1, self.df, 0], [-1, 0, 1], [0, 0, 1]])\n",
    "        return np.broadcast_to(w, (9, 3, 3))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # = torch.bmm(torch.pow(self.weights, self.num_iter), x.unsqueeze(2)).squeeze(2)\n",
    "        x = torch.matmul(self.weights, x.transpose_(0, 1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_channel):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=num_channel, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(8, 8, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(8, 8, 2)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(200, 128)\n",
    "        self.fc2 = nn.Linear(128, 3)\n",
    "        self.mmc = MMC()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = x.view(-1, 5*5*8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        print('before: ', x)\n",
    "        x = self.mmc(x)\n",
    "        print('after: ', x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([16, 1, 3, 3])\n",
      "conv1.bias torch.Size([16])\n",
      "conv2.weight torch.Size([8, 16, 3, 3])\n",
      "conv2.bias torch.Size([8])\n",
      "conv3.weight torch.Size([8, 8, 3, 3])\n",
      "conv3.bias torch.Size([8])\n",
      "conv4.weight torch.Size([8, 8, 2, 2])\n",
      "conv4.bias torch.Size([8])\n",
      "fc1.weight torch.Size([128, 200])\n",
      "fc1.bias torch.Size([128])\n",
      "fc2.weight torch.Size([3, 128])\n",
      "fc2.bias torch.Size([3])\n",
      "mmc.weights torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "model = Encoder(1)\n",
    "for n, p in model.named_parameters():\n",
    "    print(n, p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(model, (1, 100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_channel):\n",
    "    \n",
    "    net = Encoder(num_channel=num_channel)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "    \n",
    "    return net, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(x):\n",
    "    return torch.from_numpy(x).float()\n",
    "\n",
    "def get_data(height, width, num_channel, path, size, bs):\n",
    "    \n",
    "    dgen = DataGen(height, width, num_channel)\n",
    "    x, y = dgen.get_data(path, True, size, True)\n",
    "    print('Data loaded...\\nx:{}\\ty:{}\\n'.format(x.shape, y.shape))\n",
    "    \n",
    "    x = x / 255.\n",
    "    x = x[:90]\n",
    "    y = y[:90]\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, shuffle=True)\n",
    "    x_train, x_test, y_train, y_test = map(to_tensor, (x_train, x_test, y_train, y_test))\n",
    "    \n",
    "    train_ds = TensorDataset(x_train, y_train)\n",
    "    train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "\n",
    "    test_ds = TensorDataset(x_test, y_test)\n",
    "    test_dl = DataLoader(test_ds, batch_size=bs)\n",
    "    \n",
    "    return train_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_flow(named_parameters):\n",
    "    ave_grads = []\n",
    "    max_grads= []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean())\n",
    "            max_grads.append(p.grad.abs().max())\n",
    "    print('layers: {}'.format(layers))\n",
    "    print('max: {}'.format(max_grads))\n",
    "    print('mean: {}\\n\\n'.format( ave_grads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(net, optimizer, train_dl, test_dl, epochs):\n",
    "    \n",
    "    loss_function = nn.MSELoss()\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        for x, y in train_dl:\n",
    "            pred = net(x)\n",
    "            loss = loss_function(pred, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # grad_flow(net.named_parameters())\n",
    "            optimizer.step()\n",
    "\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            test_loss = sum(loss_function(net(x), y) for x, y in test_dl)\n",
    "        print('epoch:{}, test_loss:{}'.format(epoch+1, test_loss/len(test_dl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data/data_simple_movement/'\n",
    "HEIGHT = 100\n",
    "WIDTH = 100\n",
    "NUM_CHANNEL = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded...\n",
      "x:(96, 2, 100, 100)\ty:(96, 3)\n",
      "\n",
      "before:  tensor([[-0.0334,  0.0259, -0.0625]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[-0.0282],\n",
      "        [-0.0292],\n",
      "        [-0.0625]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[-0.0275,  0.0297, -0.0300]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[-0.0215],\n",
      "        [-0.0025],\n",
      "        [-0.0300]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[-0.0148,  0.0325,  0.0053]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[-0.0083],\n",
      "        [ 0.0201],\n",
      "        [ 0.0053]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.0018, 0.0360, 0.0483]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.0090],\n",
      "        [0.0465],\n",
      "        [0.0483]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.0176, 0.0413, 0.0968]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.0259],\n",
      "        [0.0792],\n",
      "        [0.0968]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.0376, 0.0469, 0.1530]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.0470],\n",
      "        [0.1154],\n",
      "        [0.1530]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.0588, 0.0569, 0.2179]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.0702],\n",
      "        [0.1590],\n",
      "        [0.2179]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.0855, 0.0712, 0.2924]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.0997],\n",
      "        [0.2069],\n",
      "        [0.2924]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.1194, 0.0911, 0.3768]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.1376],\n",
      "        [0.2574],\n",
      "        [0.3768]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.1590, 0.1172, 0.4639]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.1824],\n",
      "        [0.3050],\n",
      "        [0.4639]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.2089, 0.1496, 0.5758]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.2388],\n",
      "        [0.3669],\n",
      "        [0.5758]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.2725, 0.1903, 0.7199]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.3105],\n",
      "        [0.4474],\n",
      "        [0.7199]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.3471, 0.2389, 0.8866]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.3949],\n",
      "        [0.5395],\n",
      "        [0.8866]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4411, 0.3014, 1.0905]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.5014],\n",
      "        [0.6493],\n",
      "        [1.0905]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.5612, 0.3824, 1.3456]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.6377],\n",
      "        [0.7844],\n",
      "        [1.3456]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.6595, 0.4492, 1.5094]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.7493],\n",
      "        [0.8499],\n",
      "        [1.5094]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.7898, 0.5386, 1.7402]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.8975],\n",
      "        [0.9504],\n",
      "        [1.7402]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.8729, 0.5943, 1.8401]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.9918],\n",
      "        [0.9672],\n",
      "        [1.8401]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.9794, 0.6734, 1.9998]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[1.1140],\n",
      "        [1.0204],\n",
      "        [1.9998]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[1.0644, 0.7532, 2.1344]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[1.2150],\n",
      "        [1.0700],\n",
      "        [2.1344]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[1.1227, 0.8330, 2.2468]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[1.2893],\n",
      "        [1.1241],\n",
      "        [2.2468]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[1.1274, 0.8832, 2.2728]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[1.3040],\n",
      "        [1.1454],\n",
      "        [2.2728]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[1.1016, 0.9197, 2.2580]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[1.2856],\n",
      "        [1.1564],\n",
      "        [2.2580]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[1.0859, 0.9722, 2.2804]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[1.2803],\n",
      "        [1.1945],\n",
      "        [2.2804]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[1.0418, 1.0034, 2.2408]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[1.2425],\n",
      "        [1.1990],\n",
      "        [2.2408]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.9366, 0.9536, 2.0461]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[1.1273],\n",
      "        [1.1095],\n",
      "        [2.0461]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.8136, 0.8702, 1.7954]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.9876],\n",
      "        [0.9818],\n",
      "        [1.7954]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.7278, 0.8186, 1.6252]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.8915],\n",
      "        [0.8975],\n",
      "        [1.6252]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.6595, 0.7807, 1.4863]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.8156],\n",
      "        [0.8268],\n",
      "        [1.4863]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.5986, 0.7423, 1.3563]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.7470],\n",
      "        [0.7578],\n",
      "        [1.3563]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.5375, 0.6954, 1.2168]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.6766],\n",
      "        [0.6793],\n",
      "        [1.2168]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4863, 0.6548, 1.0996]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.6172],\n",
      "        [0.6133],\n",
      "        [1.0996]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4523, 0.6323, 1.0269]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.5788],\n",
      "        [0.5746],\n",
      "        [1.0269]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4222, 0.6111, 0.9603]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.5444],\n",
      "        [0.5381],\n",
      "        [0.9603]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.3964, 0.5936, 0.9047]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.5152],\n",
      "        [0.5083],\n",
      "        [0.9047]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.3818, 0.5905, 0.8784]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.4999],\n",
      "        [0.4967],\n",
      "        [0.8784]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.3758, 0.6003, 0.8754]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.4959],\n",
      "        [0.4996],\n",
      "        [0.8754]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.3792, 0.6227, 0.8947]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.5037],\n",
      "        [0.5155],\n",
      "        [0.8947]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.3844, 0.6480, 0.9219]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.5141],\n",
      "        [0.5375],\n",
      "        [0.9219]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.3988, 0.6870, 0.9703]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.5362],\n",
      "        [0.5715],\n",
      "        [0.9703]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4137, 0.7266, 1.0195]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.5590],\n",
      "        [0.6057],\n",
      "        [1.0195]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4269, 0.7610, 1.0593]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.5791],\n",
      "        [0.6324],\n",
      "        [1.0593]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4411, 0.7960, 1.0970]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.6003],\n",
      "        [0.6559],\n",
      "        [1.0970]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4515, 0.8211, 1.1190]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.6158],\n",
      "        [0.6674],\n",
      "        [1.1190]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4583, 0.8377, 1.1270]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.6259],\n",
      "        [0.6686],\n",
      "        [1.1270]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4608, 0.8444, 1.1195]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.6297],\n",
      "        [0.6587],\n",
      "        [1.1195]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4580, 0.8402, 1.0953]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.6260],\n",
      "        [0.6373],\n",
      "        [1.0953]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4496, 0.8254, 1.0562]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.6146],\n",
      "        [0.6067],\n",
      "        [1.0562]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4373, 0.8037, 1.0076]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.5981],\n",
      "        [0.5703],\n",
      "        [1.0076]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4300, 0.7941, 0.9790]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.5888],\n",
      "        [0.5490],\n",
      "        [0.9790]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4199, 0.7805, 0.9457]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.5760],\n",
      "        [0.5258],\n",
      "        [0.9457]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4182, 0.7840, 0.9373]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.5750],\n",
      "        [0.5191],\n",
      "        [0.9373]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4197, 0.7968, 0.9432]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.5790],\n",
      "        [0.5235],\n",
      "        [0.9432]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4196, 0.8085, 0.9487]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.5813],\n",
      "        [0.5291],\n",
      "        [0.9487]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4184, 0.8205, 0.9539]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.5825],\n",
      "        [0.5355],\n",
      "        [0.9539]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4213, 0.8416, 0.9726]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.5896],\n",
      "        [0.5513],\n",
      "        [0.9726]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4257, 0.8683, 1.0004]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.5994],\n",
      "        [0.5746],\n",
      "        [1.0004]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4415, 0.9184, 1.0573]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.6252],\n",
      "        [0.6157],\n",
      "        [1.0573]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4522, 0.9603, 1.1046]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.6443],\n",
      "        [0.6524],\n",
      "        [1.1046]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4719, 1.0198, 1.1724]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.6758],\n",
      "        [0.7006],\n",
      "        [1.1724]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4829, 1.0589, 1.2125]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.6946],\n",
      "        [0.7297],\n",
      "        [1.2125]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4916, 1.0922, 1.2441]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.7100],\n",
      "        [0.7525],\n",
      "        [1.2441]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.5133, 1.1506, 1.3050]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.7434],\n",
      "        [0.7918],\n",
      "        [1.3050]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.5313, 1.1982, 1.3530]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.7709],\n",
      "        [0.8218],\n",
      "        [1.3530]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.5590, 1.2625, 1.4177]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.8115],\n",
      "        [0.8587],\n",
      "        [1.4177]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.5954, 1.3447, 1.5036]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.8643],\n",
      "        [0.9082],\n",
      "        [1.5036]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.6172, 1.3900, 1.5409]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.8952],\n",
      "        [0.9237],\n",
      "        [1.5409]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.6214, 1.3922, 1.5230]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.8998],\n",
      "        [0.9016],\n",
      "        [1.5230]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.6145, 1.3725, 1.4782]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.8890],\n",
      "        [0.8637],\n",
      "        [1.4782]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.6216, 1.3895, 1.4780]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.8995],\n",
      "        [0.8564],\n",
      "        [1.4780]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.6325, 1.4216, 1.4953]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.9169],\n",
      "        [0.8627],\n",
      "        [1.4953]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.6276, 1.4231, 1.4768]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.9122],\n",
      "        [0.8492],\n",
      "        [1.4768]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.6183, 1.4246, 1.4582]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.9033],\n",
      "        [0.8399],\n",
      "        [1.4582]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.6200, 1.4573, 1.4763]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.9114],\n",
      "        [0.8563],\n",
      "        [1.4763]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.6265, 1.5069, 1.5124]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.9279],\n",
      "        [0.8860],\n",
      "        [1.5124]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.6315, 1.5599, 1.5526]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.9434],\n",
      "        [0.9211],\n",
      "        [1.5526]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.6297, 1.5993, 1.5778]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.9495],\n",
      "        [0.9481],\n",
      "        [1.5778]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.6311, 1.6442, 1.6043]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.9600],\n",
      "        [0.9732],\n",
      "        [1.6043]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.6172, 1.6452, 1.5827]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.9462],\n",
      "        [0.9655],\n",
      "        [1.5827]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.5923, 1.6117, 1.5240]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.9147],\n",
      "        [0.9317],\n",
      "        [1.5240]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.5699, 1.5784, 1.4654]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.8856],\n",
      "        [0.8954],\n",
      "        [1.4654]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.5509, 1.5509, 1.4134]])\n",
      "after:  tensor([[0.8611],\n",
      "        [0.8624],\n",
      "        [1.4134]])\n",
      "before:  tensor([[0.5528, 1.5553, 1.4169]])\n",
      "after:  tensor([[0.8639],\n",
      "        [0.8641],\n",
      "        [1.4169]])\n",
      "before:  tensor([[0.5516, 1.5531, 1.4158]])\n",
      "after:  tensor([[0.8623],\n",
      "        [0.8642],\n",
      "        [1.4158]])\n",
      "before:  tensor([[0.5517, 1.5536, 1.4165]])\n",
      "after:  tensor([[0.8624],\n",
      "        [0.8648],\n",
      "        [1.4165]])\n",
      "before:  tensor([[0.5528, 1.5552, 1.4177]])\n",
      "after:  tensor([[0.8638],\n",
      "        [0.8650],\n",
      "        [1.4177]])\n",
      "before:  tensor([[0.5520, 1.5538, 1.4167]])\n",
      "after:  tensor([[0.8627],\n",
      "        [0.8647],\n",
      "        [1.4167]])\n",
      "before:  tensor([[0.5523, 1.5543, 1.4168]])\n",
      "after:  tensor([[0.8632],\n",
      "        [0.8644],\n",
      "        [1.4168]])\n",
      "before:  tensor([[0.5514, 1.5522, 1.4144]])\n",
      "after:  tensor([[0.8619],\n",
      "        [0.8630],\n",
      "        [1.4144]])\n",
      "before:  tensor([[0.5522, 1.5540, 1.4159]])\n",
      "after:  tensor([[0.8630],\n",
      "        [0.8637],\n",
      "        [1.4159]])\n",
      "epoch:1, test_loss:1.071455478668213\n",
      "before:  tensor([[0.5521, 1.5535, 1.4159]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.8628],\n",
      "        [0.8638],\n",
      "        [1.4159]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.5281, 1.5094, 1.3476]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.8300],\n",
      "        [0.8195],\n",
      "        [1.3476]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.5063, 1.4713, 1.2877]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.8005],\n",
      "        [0.7815],\n",
      "        [1.2877]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4981, 1.4712, 1.2686]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.7924],\n",
      "        [0.7705],\n",
      "        [1.2686]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4860, 1.4621, 1.2407]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.7784],\n",
      "        [0.7547],\n",
      "        [1.2407]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4693, 1.4404, 1.2023]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.7573],\n",
      "        [0.7331],\n",
      "        [1.2023]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4614, 1.4447, 1.1898]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.7504],\n",
      "        [0.7284],\n",
      "        [1.1898]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4483, 1.4349, 1.1646]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.7353],\n",
      "        [0.7163],\n",
      "        [1.1646]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4309, 1.4096, 1.1232]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.7128],\n",
      "        [0.6923],\n",
      "        [1.1232]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4179, 1.3988, 1.0989]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.6976],\n",
      "        [0.6811],\n",
      "        [1.0989]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4014, 1.3765, 1.0647]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.6767],\n",
      "        [0.6633],\n",
      "        [1.0647]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.3994, 1.4006, 1.0753]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.6795],\n",
      "        [0.6759],\n",
      "        [1.0753]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4036, 1.4416, 1.1005]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.6919],\n",
      "        [0.6969],\n",
      "        [1.1005]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4115, 1.4970, 1.1420]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.7109],\n",
      "        [0.7304],\n",
      "        [1.1420]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4319, 1.5910, 1.2171]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.7501],\n",
      "        [0.7852],\n",
      "        [1.2171]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4470, 1.6627, 1.2714]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.7796],\n",
      "        [0.8243],\n",
      "        [1.2714]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4692, 1.7496, 1.3372]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.8191],\n",
      "        [0.8680],\n",
      "        [1.3372]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4894, 1.8221, 1.3892]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.8538],\n",
      "        [0.8999],\n",
      "        [1.3892]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.5044, 1.8683, 1.4160]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.8781],\n",
      "        [0.9116],\n",
      "        [1.4160]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.5155, 1.8940, 1.4225]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.8943],\n",
      "        [0.9070],\n",
      "        [1.4225]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.5115, 1.8654, 1.3806]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.8846],\n",
      "        [0.8691],\n",
      "        [1.3806]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4954, 1.8002, 1.3067]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.8554],\n",
      "        [0.8113],\n",
      "        [1.3067]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4850, 1.7669, 1.2638]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.8383],\n",
      "        [0.7789],\n",
      "        [1.2638]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4769, 1.7532, 1.2380]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.8275],\n",
      "        [0.7612],\n",
      "        [1.2380]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4753, 1.7731, 1.2406]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.8299],\n",
      "        [0.7653],\n",
      "        [1.2406]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4791, 1.8257, 1.2741]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.8442],\n",
      "        [0.7951],\n",
      "        [1.2741]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.4890, 1.9107, 1.3344]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.8711],\n",
      "        [0.8454],\n",
      "        [1.3344]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.5025, 2.0140, 1.4084]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.9053],\n",
      "        [0.9059],\n",
      "        [1.4084]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.5234, 2.1471, 1.5071]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[0.9529],\n",
      "        [0.9836],\n",
      "        [1.5071]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.5544, 2.3128, 1.6302]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[1.0170],\n",
      "        [1.0758],\n",
      "        [1.6302]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.5713, 2.4040, 1.6894]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[1.0521],\n",
      "        [1.1181],\n",
      "        [1.6894]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.5921, 2.4896, 1.7413]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[1.0900],\n",
      "        [1.1492],\n",
      "        [1.7413]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.5948, 2.4835, 1.7173]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[1.0914],\n",
      "        [1.1226],\n",
      "        [1.7173]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.5989, 2.4745, 1.6885]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[1.0938],\n",
      "        [1.0896],\n",
      "        [1.6885]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.6136, 2.5133, 1.6972]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[1.1163],\n",
      "        [1.0836],\n",
      "        [1.6972]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.6303, 2.5785, 1.7292]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[1.1460],\n",
      "        [1.0989],\n",
      "        [1.7292]], grad_fn=<MmBackward>)\n",
      "before:  tensor([[0.6470, 2.6619, 1.7716]], grad_fn=<AddmmBackward>)\n",
      "after:  tensor([[1.1794],\n",
      "        [1.1246],\n",
      "        [1.7716]], grad_fn=<MmBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-f388294e9f7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-5e4a9ffe04c1>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(net, optimizer, train_dl, test_dl, epochs)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\dl\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-0684ad122f45>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\dl\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\dl\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 320\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net, optimizer = get_model(num_channel=2)\n",
    "\n",
    "train_dl, test_dl = get_data(\n",
    "    height=HEIGHT, \n",
    "    width=WIDTH,\n",
    "    num_channel=NUM_CHANNEL,\n",
    "    path=DATA_DIR,\n",
    "    size=8,\n",
    "    bs=1\n",
    ")\n",
    "\n",
    "fit(net, optimizer, train_dl, test_dl, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: tensor([[1.5738],\n",
      "        [1.5649],\n",
      "        [1.5988]], grad_fn=<MmBackward>), target: tensor([[2.3503, 0.0082, 2.3562]])\n",
      "prediction: tensor([[0.3144],\n",
      "        [0.3077],\n",
      "        [0.3060]], grad_fn=<MmBackward>), target: tensor([[0.7834, 0.0027, 0.7854]])\n",
      "prediction: tensor([[1.0492],\n",
      "        [1.0388],\n",
      "        [1.0842]], grad_fn=<MmBackward>), target: tensor([[1.5669, 0.0054, 1.5708]])\n",
      "prediction: tensor([[1.5840],\n",
      "        [1.5703],\n",
      "        [1.6074]], grad_fn=<MmBackward>), target: tensor([[2.3503, 0.0082, 2.3562]])\n",
      "prediction: tensor([[0.5493],\n",
      "        [0.5370],\n",
      "        [0.5649]], grad_fn=<MmBackward>), target: tensor([[1.0446, 0.0036, 1.0472]])\n",
      "prediction: tensor([[0.1723],\n",
      "        [0.1767],\n",
      "        [0.1480]], grad_fn=<MmBackward>), target: tensor([[0.2611, 0.0009, 0.2618]])\n",
      "prediction: tensor([[0.3171],\n",
      "        [0.3109],\n",
      "        [0.3056]], grad_fn=<MmBackward>), target: tensor([[0.7834, 0.0027, 0.7854]])\n",
      "prediction: tensor([[1.5730],\n",
      "        [1.5656],\n",
      "        [1.6028]], grad_fn=<MmBackward>), target: tensor([[2.6114, 0.0091, 2.6180]])\n",
      "prediction: tensor([[0.8186],\n",
      "        [0.8084],\n",
      "        [0.8551]], grad_fn=<MmBackward>), target: tensor([[1.3057, 0.0045, 1.3090]])\n"
     ]
    }
   ],
   "source": [
    "for x, y in test_dl:\n",
    "    print('prediction: {}, target: {}'.format(net(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04966],\n",
       "       [ 0.0709 ],\n",
       "       [ 0.0237 ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[-0.0472], [-0.0123],  [0.0237]])\n",
    "b = np.array([[1, 0.2, 0], [-1, 0, 1], [0, 0, 1]])\n",
    "np.dot(b, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         iteration = 0\n",
    "#         while iteration < self.num_iter:\n",
    "#             for i in range(x.shape[0]):\n",
    "#                 x[i] = torch.matmul(self.weights, x[i])\n",
    "#             iteration += 1\n",
    "        # x = torch.acos(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2, 3]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
