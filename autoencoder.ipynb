{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Lambda, Reshape, Flatten, ZeroPadding2D, Layer\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from data_generator import DataGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data/data_simple_movement/'\n",
    "HEIGHT = 100\n",
    "WIDTH = 100\n",
    "NUM_CHANNEL = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100, 100, 2)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 100, 100, 16)      304       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 50, 50, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 50, 50, 8)         1160      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 25, 25, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 25, 25, 8)         584       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 13, 13, 8)         584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 26, 26, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 26, 26, 8)         584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 52, 52, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 50, 50, 16)        1168      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 100, 100, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 100, 100, 1)       145       \n",
      "=================================================================\n",
      "Total params: 4,529\n",
      "Trainable params: 4,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_img = Input(shape=(HEIGHT, WIDTH, NUM_CHANNEL))\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 86 samples, validate on 10 samples\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.6446 - val_loss: 0.5313\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 2s 28ms/step - loss: 0.3574 - val_loss: 0.1780\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 2s 29ms/step - loss: 0.2140 - val_loss: 0.2058\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 3s 30ms/step - loss: 0.1820 - val_loss: 0.1777\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 2s 28ms/step - loss: 0.1737 - val_loss: 0.1673\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 2s 29ms/step - loss: 0.1646 - val_loss: 0.1610\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 2s 28ms/step - loss: 0.1589 - val_loss: 0.1561\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 3s 32ms/step - loss: 0.1538 - val_loss: 0.1508\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 3s 32ms/step - loss: 0.1484 - val_loss: 0.1451\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 3s 31ms/step - loss: 0.1421 - val_loss: 0.1382\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 3s 32ms/step - loss: 0.1353 - val_loss: 0.1314\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 3s 30ms/step - loss: 0.1283 - val_loss: 0.1237\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 3s 31ms/step - loss: 0.1200 - val_loss: 0.1145\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 2s 28ms/step - loss: 0.1101 - val_loss: 0.1045\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 3s 30ms/step - loss: 0.1002 - val_loss: 0.0948\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 3s 31ms/step - loss: 0.0911 - val_loss: 0.0861\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0822 - val_loss: 0.0769\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 5s 53ms/step - loss: 0.0730 - val_loss: 0.0683\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 5s 53ms/step - loss: 0.0650 - val_loss: 0.0609\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 4s 49ms/step - loss: 0.0577 - val_loss: 0.0545\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 5s 59ms/step - loss: 0.0516 - val_loss: 0.0480\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 5s 60ms/step - loss: 0.0455 - val_loss: 0.0418\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 5s 59ms/step - loss: 0.0387 - val_loss: 0.0351\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 6s 64ms/step - loss: 0.0322 - val_loss: 0.0292\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 6s 64ms/step - loss: 0.0283 - val_loss: 0.0264\n",
      "Epoch 26/100\n",
      "86/86 [==============================] - 5s 62ms/step - loss: 0.0276 - val_loss: 0.0271\n",
      "Epoch 27/100\n",
      "86/86 [==============================] - 5s 60ms/step - loss: 0.0253 - val_loss: 0.0244\n",
      "Epoch 28/100\n",
      "86/86 [==============================] - 5s 59ms/step - loss: 0.0240 - val_loss: 0.0251\n",
      "Epoch 29/100\n",
      "86/86 [==============================] - 5s 57ms/step - loss: 0.0239 - val_loss: 0.0234\n",
      "Epoch 30/100\n",
      "86/86 [==============================] - 5s 61ms/step - loss: 0.0230 - val_loss: 0.0242\n",
      "Epoch 31/100\n",
      "86/86 [==============================] - 7s 76ms/step - loss: 0.0237 - val_loss: 0.0235\n",
      "Epoch 32/100\n",
      "86/86 [==============================] - 6s 67ms/step - loss: 0.0229 - val_loss: 0.0228\n",
      "Epoch 33/100\n",
      "86/86 [==============================] - 6s 72ms/step - loss: 0.0222 - val_loss: 0.0222\n",
      "Epoch 34/100\n",
      "86/86 [==============================] - 6s 74ms/step - loss: 0.0220 - val_loss: 0.0220\n",
      "Epoch 35/100\n",
      "86/86 [==============================] - 6s 71ms/step - loss: 0.0218 - val_loss: 0.0218\n",
      "Epoch 36/100\n",
      "86/86 [==============================] - 6s 75ms/step - loss: 0.0218 - val_loss: 0.0218\n",
      "Epoch 37/100\n",
      "86/86 [==============================] - 7s 80ms/step - loss: 0.0217 - val_loss: 0.0216\n",
      "Epoch 38/100\n",
      "86/86 [==============================] - 6s 69ms/step - loss: 0.0216 - val_loss: 0.0216\n",
      "Epoch 39/100\n",
      "86/86 [==============================] - 7s 79ms/step - loss: 0.0215 - val_loss: 0.0220\n",
      "Epoch 40/100\n",
      "86/86 [==============================] - 6s 69ms/step - loss: 0.0220 - val_loss: 0.0220\n",
      "Epoch 41/100\n",
      "86/86 [==============================] - 6s 68ms/step - loss: 0.0220 - val_loss: 0.0217\n",
      "Epoch 42/100\n",
      "86/86 [==============================] - 6s 69ms/step - loss: 0.0215 - val_loss: 0.0212\n",
      "Epoch 43/100\n",
      "86/86 [==============================] - 6s 70ms/step - loss: 0.0212 - val_loss: 0.0212\n",
      "Epoch 44/100\n",
      "86/86 [==============================] - 6s 66ms/step - loss: 0.0211 - val_loss: 0.0211\n",
      "Epoch 45/100\n",
      "86/86 [==============================] - 6s 69ms/step - loss: 0.0211 - val_loss: 0.0211\n",
      "Epoch 46/100\n",
      "86/86 [==============================] - 6s 68ms/step - loss: 0.0211 - val_loss: 0.0213\n",
      "Epoch 47/100\n",
      "86/86 [==============================] - 6s 68ms/step - loss: 0.0211 - val_loss: 0.0212\n",
      "Epoch 48/100\n",
      "86/86 [==============================] - 6s 70ms/step - loss: 0.0210 - val_loss: 0.0213\n",
      "Epoch 49/100\n",
      "86/86 [==============================] - 6s 66ms/step - loss: 0.0211 - val_loss: 0.0214\n",
      "Epoch 50/100\n",
      "86/86 [==============================] - 6s 67ms/step - loss: 0.0211 - val_loss: 0.0209\n",
      "Epoch 51/100\n",
      "86/86 [==============================] - 6s 70ms/step - loss: 0.0209 - val_loss: 0.0209\n",
      "Epoch 52/100\n",
      "86/86 [==============================] - 6s 68ms/step - loss: 0.0209 - val_loss: 0.0209\n",
      "Epoch 53/100\n",
      "86/86 [==============================] - 6s 70ms/step - loss: 0.0208 - val_loss: 0.0210\n",
      "Epoch 54/100\n",
      "86/86 [==============================] - 6s 69ms/step - loss: 0.0210 - val_loss: 0.0208\n",
      "Epoch 55/100\n",
      "86/86 [==============================] - 6s 70ms/step - loss: 0.0208 - val_loss: 0.0209\n",
      "Epoch 56/100\n",
      "86/86 [==============================] - 6s 70ms/step - loss: 0.0209 - val_loss: 0.0207\n",
      "Epoch 57/100\n",
      "86/86 [==============================] - 6s 69ms/step - loss: 0.0208 - val_loss: 0.0206\n",
      "Epoch 58/100\n",
      "86/86 [==============================] - 6s 70ms/step - loss: 0.0206 - val_loss: 0.0207\n",
      "Epoch 59/100\n",
      "86/86 [==============================] - 6s 75ms/step - loss: 0.0206 - val_loss: 0.0206\n",
      "Epoch 60/100\n",
      "86/86 [==============================] - 6s 75ms/step - loss: 0.0207 - val_loss: 0.0208\n",
      "Epoch 61/100\n",
      "86/86 [==============================] - 7s 80ms/step - loss: 0.0206 - val_loss: 0.0205\n",
      "Epoch 62/100\n",
      "86/86 [==============================] - 6s 74ms/step - loss: 0.0205 - val_loss: 0.0205\n",
      "Epoch 63/100\n",
      "86/86 [==============================] - 6s 71ms/step - loss: 0.0205 - val_loss: 0.0208\n",
      "Epoch 64/100\n",
      "86/86 [==============================] - 6s 71ms/step - loss: 0.0206 - val_loss: 0.0205\n",
      "Epoch 65/100\n",
      "86/86 [==============================] - 6s 70ms/step - loss: 0.0205 - val_loss: 0.0204\n",
      "Epoch 66/100\n",
      "86/86 [==============================] - 6s 69ms/step - loss: 0.0204 - val_loss: 0.0204\n",
      "Epoch 67/100\n",
      "86/86 [==============================] - 6s 69ms/step - loss: 0.0204 - val_loss: 0.0205\n",
      "Epoch 68/100\n",
      "86/86 [==============================] - 6s 70ms/step - loss: 0.0204 - val_loss: 0.0205\n",
      "Epoch 69/100\n",
      "86/86 [==============================] - 6s 69ms/step - loss: 0.0204 - val_loss: 0.0203\n",
      "Epoch 70/100\n",
      "86/86 [==============================] - 6s 72ms/step - loss: 0.0204 - val_loss: 0.0205\n",
      "Epoch 71/100\n",
      "86/86 [==============================] - 6s 69ms/step - loss: 0.0204 - val_loss: 0.0204\n",
      "Epoch 72/100\n",
      "86/86 [==============================] - 6s 66ms/step - loss: 0.0205 - val_loss: 0.0206\n",
      "Epoch 73/100\n",
      "86/86 [==============================] - 6s 69ms/step - loss: 0.0205 - val_loss: 0.0203\n",
      "Epoch 74/100\n",
      "86/86 [==============================] - 6s 72ms/step - loss: 0.0204 - val_loss: 0.0203\n",
      "Epoch 75/100\n",
      "86/86 [==============================] - 6s 71ms/step - loss: 0.0204 - val_loss: 0.0204\n",
      "Epoch 76/100\n",
      "86/86 [==============================] - 6s 68ms/step - loss: 0.0205 - val_loss: 0.0205\n",
      "Epoch 77/100\n",
      "86/86 [==============================] - 6s 68ms/step - loss: 0.0203 - val_loss: 0.0203\n",
      "Epoch 78/100\n",
      "86/86 [==============================] - 6s 68ms/step - loss: 0.0203 - val_loss: 0.0207\n",
      "Epoch 79/100\n",
      "86/86 [==============================] - 6s 74ms/step - loss: 0.0203 - val_loss: 0.0202\n",
      "Epoch 80/100\n",
      "86/86 [==============================] - 7s 76ms/step - loss: 0.0201 - val_loss: 0.0202\n",
      "Epoch 81/100\n",
      "86/86 [==============================] - 7s 76ms/step - loss: 0.0201 - val_loss: 0.0202\n",
      "Epoch 82/100\n",
      "86/86 [==============================] - 6s 66ms/step - loss: 0.0201 - val_loss: 0.0201\n",
      "Epoch 83/100\n",
      "86/86 [==============================] - 6s 73ms/step - loss: 0.0201 - val_loss: 0.0201\n",
      "Epoch 84/100\n",
      "86/86 [==============================] - 6s 69ms/step - loss: 0.0201 - val_loss: 0.0202\n",
      "Epoch 85/100\n",
      "86/86 [==============================] - 6s 66ms/step - loss: 0.0201 - val_loss: 0.0203\n",
      "Epoch 86/100\n",
      "86/86 [==============================] - 6s 69ms/step - loss: 0.0201 - val_loss: 0.0203\n",
      "Epoch 87/100\n",
      "86/86 [==============================] - 6s 68ms/step - loss: 0.0203 - val_loss: 0.0202\n",
      "Epoch 88/100\n",
      "86/86 [==============================] - 6s 68ms/step - loss: 0.0203 - val_loss: 0.0204\n",
      "Epoch 89/100\n",
      "86/86 [==============================] - 6s 71ms/step - loss: 0.0202 - val_loss: 0.0201\n",
      "Epoch 90/100\n",
      "86/86 [==============================] - 6s 69ms/step - loss: 0.0203 - val_loss: 0.0203\n",
      "Epoch 91/100\n",
      "86/86 [==============================] - 6s 71ms/step - loss: 0.0201 - val_loss: 0.0201\n",
      "Epoch 92/100\n",
      "86/86 [==============================] - 6s 72ms/step - loss: 0.0201 - val_loss: 0.0201\n",
      "Epoch 93/100\n",
      "86/86 [==============================] - 6s 72ms/step - loss: 0.0199 - val_loss: 0.0202\n",
      "Epoch 94/100\n",
      "86/86 [==============================] - 6s 69ms/step - loss: 0.0199 - val_loss: 0.0202\n",
      "Epoch 95/100\n",
      "86/86 [==============================] - 6s 74ms/step - loss: 0.0199 - val_loss: 0.0201\n",
      "Epoch 96/100\n",
      "86/86 [==============================] - 6s 72ms/step - loss: 0.0200 - val_loss: 0.0202\n",
      "Epoch 97/100\n",
      "86/86 [==============================] - 6s 71ms/step - loss: 0.0199 - val_loss: 0.0200\n",
      "Epoch 98/100\n",
      "86/86 [==============================] - 6s 69ms/step - loss: 0.0198 - val_loss: 0.0199\n",
      "Epoch 99/100\n",
      "86/86 [==============================] - 6s 73ms/step - loss: 0.0198 - val_loss: 0.0200\n",
      "Epoch 100/100\n",
      "86/86 [==============================] - 6s 71ms/step - loss: 0.0198 - val_loss: 0.0202\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x229d55e8c50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgen = DataGen(HEIGHT, WIDTH, NUM_CHANNEL)\n",
    "x, y = dgen.get_data(path=os.path.abspath(DATA_DIR),\n",
    "                     target_mmc_out=False,\n",
    "                     size=8,\n",
    "                     channel_first=False)\n",
    "\n",
    "x = x / 255.\n",
    "y = y / 255.\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, shuffle=True)\n",
    "\n",
    "autoencoder.fit(x_train, y_train,\n",
    "                epochs=100,\n",
    "                batch_size=10,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs = autoencoder.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 7\n",
    "\n",
    "fig, axes = plt.subplots(figsize=((10, 3)), nrows=1, ncols=NUM_CHANNEL+2)\n",
    "\n",
    "# display input images\n",
    "for i, ax in enumerate(axes.flat[:NUM_CHANNEL]):\n",
    "    ax.imshow(x_test[idx][:, :, i].reshape(HEIGHT, WIDTH), cmap='gray')\n",
    "    ax.axis('off'), ax.set_title('step ' + str(i+1))\n",
    "    \n",
    "# display prediction\n",
    "axes[NUM_CHANNEL].imshow(decoded_imgs[idx][:, :, 0].reshape(HEIGHT, WIDTH), cmap='gray')\n",
    "axes[NUM_CHANNEL].axis('off'), axes[NUM_CHANNEL].set_title('predicted')\n",
    "\n",
    "# display target\n",
    "axes[NUM_CHANNEL+1].imshow(y_test[idx][:, :, 0].reshape(HEIGHT, WIDTH), cmap='gray')\n",
    "axes[NUM_CHANNEL+1].axis('off'), axes[NUM_CHANNEL+1].set_title('target')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
