This project deals with the issue of extending an existing body model in order to drive the model through visual inputs. The idea behind this is to enable an agent learn the mapping between visual representations and it's internal body model by observing another agent equipped with same underlying model performing body movements.

#### MMC Network
The body model is modeled as a MMC network, a type of recurrent neural network based on Mean of Multiple Computation (MMC) principle. In the context of this project, the MMC network models movement of a three segmented arm. For details about the underlying principle of MMC network as a body model please refer to [Schilling, M.](https://link.springer.com/article/10.1007/s10514-011-9226-3)

#### Extending Body Model
The idea is to embed the MMC network into a convolutional-deconvolutional network or autoencoder. One way to accomplish this is to have the MMC network as an intermediary layer between the encoder and decoder of an autoencoder. This way, the feature representations learned from input images will be input to the MMC network, which will then be processed according to the dynamics of the MMC network to predict next arm configuration and reconstructed through the decoder.

#### Simple Movement
SimpleMovement models the dynamics of a single segmented arm in a two dimensional space. Given an initial position described in terms of angular displacement with respect to the coordinate system and initial velocity, aim is to reach a target position also described in terms of angular displacement. In essence, simple movement mimics the behavior of the MMC network but in a simpler setting. Primary reason for this is to better understand and analyze the behavior of the autoencoder when trained to predict the next image on a sequence of images as MMC network has been empirically found to be difficult to analyze.

#### Usage
##### Data generation
Training data for autoencoder can be generated by simulating the MMC/SimpleMovement network using the `DataGen` class for different targets given an initial configuration. Configurations are defined using JSON format. `config_mmc.json` and `config_simple_movement.json` contains example configuration for MMC and SimpleMovement respectively. As an example, data for MMC network can be generated using following code snippet:
```
dgen = DataGen(100, 100, 2)
dgen.generate('mmc', <path to saving directory>)
```
Parameters supplied to `DataGen` are the height, width and number of channels of the input image. For each target the network will be simulated for a fixed number of iterations and for each iteration corresponding arm configuration/position will be stored as a PNG image in the directory corresponding to the target. Example dataset can be found in the `data` folder.

##### Autoencoder 
The notebooks `autoencoder.ipynb` and `autoencoder_pytorch.ipynb` implements simple autoencoder to predict next image in the sequence given previous images. The purpose is to evaluate how well an autoencoder performs in predicting the next arm position given previous arm positions without having any knowledge of the underlying body model.


##### Autoencoder with body model
The notebook `extended_body_model.ipynb` extends simple autoencoder mentioned in the previous section with a latent layer implementing the body model. The latent layer performs one step or iteration of the MMC/SimpleMovement network on its input. Therefore, ideally it should be able to predict the next position of the arm more accurately compared to simple autoencoder.

`extended_body_model_pytorch.ipynb` takes somewhat different approach in modeling the task. Here, only the encoder portion of an autoencoder is utilized and rather than predicting the next position of the arm and reconstructing the image, the network tries to learn the final arm position associated with the input images. As mentioned earlier, for a given target or final position number of images generated is equal to the number iterations. Therefore, in this scenario, different inputs can share the same target. The reason behind this approach is two fold. First, to compare the learned representation with the target value in lower dimensional space as it is modeled by the body model. Second, to observe how well the final arm position can be predicted by iterating sufficient number of times through the body model. The task of predicting next position in the sequence is implicit in this formulation as a network which is able to predict the final arm position will also be able to predict any position in the sequence. However, when trained on the dataset, test loss was quickly saturated and predicted targets were off from the actual targets by some margin.

The reason predicting the final arm position did not produce expected result can be attributed to how SimpleMovement has been modeled. As input SimpleMovement takes starting angle of the arm with respect to the X-axis, an initial velocity and a target angle. Though it is very difficult to discern exactly what the learned representation encodes, it is more likely to encode the position of the arm in terms of some point in the coordinate space rather than angle. On the other hand, target information is not present in input images, but, the body model requires target as an input. Having the target information is quite crucial for the model(SimpleMovement) as it adjusts the velocity parameter with respect to the target.

Therefore, in order to predict the next position of the arm given previous positions it is of high importance that the target information be encoded in the learned representation that is being fed to the body model. Also, the body model should model arm position with points in coordinate space rather than angular displacement.